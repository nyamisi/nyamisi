{
  "hash": "92bd375a69930c69b58685b1d133f9a2",
  "result": {
    "markdown": "---\ntitle: \"Machine Learning (ML) using Regression Algorithm in R\"\nauthor: \"Nyamisi Peter\"\ndate: \"2023-03-13\"\ncategories: [code, ML & AI]\nimage: \"randomForestscreenShort.jpg\"\nbibliography: \"../blog.bib\"\ncode-line-numbers: true\n\n---\n\n\n# Introduction\n\nIn the previous  [post](https://nyamisi.github.io/nyamisi/posts/machine-learning-classification/) we saw how classification algorithm may be applied in machine learning (ML). In this post we are going to look the application of regression algorithm in machine learning. We will use penguins data from the **palmerpenguins** package [@penguin20]. It includes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\nFirst, we will load the packages which we are going to use in this post; I will use `require()` function, but you may also use `library()` function depending on your preferences. The `tidyverse` is a collection of R packages that are designed to work together to make data manipulation and analysis easier and more efficient. Tidyverse is used as a workflow from data preprocessing to model fitting and evaluation. The packages in the Tidyverse, such as `dplyr` and `ggplot2`, will be used in data manipulation and plotting of graphs respectively. \n\n`Tidymodels`, on the other hand, is a collection of R packages for modeling and machine learning tasks. Tidymodels includes packages such as `rsample` for the initial splitting and training of the data, `parsnip` for specifying and fitting models, and `yardstick` for evaluating model performance. The `ranger` package will be used in setting the engine in **random forest** model.\n\nThe data used will come from `palmerpenguins` package. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(tidyverse)\nrequire(tidymodels)\nrequire(ranger)\nrequire(palmerpenguins)\n```\n:::\n\n\n## Data loading and preprocessing\n\nAfter loading the packages, load the penguins data [@penguin20] and remove all the missing values in the dataset. Name the dataset as ***penguin.data***. The dataset consist of 333 rows and 8 columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin.data = palmerpenguins::penguins %>% \n  drop_na()\n\npenguin.data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 323 more rows\n# ℹ 2 more variables: sex <fct>, year <int>\n```\n:::\n:::\n\n\nThen, observe the internal structure of the dataset using `glimpse()` function of **dplyr** package. The dataset consist of 8 variables (columns); 3 are factors data (species, island and sex), 2 numeric or double data (bill_length_mm, bill_depth_mm) and 3 integers (flipper_length_mm, body_mass_g, year).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(penguin.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 333\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm <int> 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       <int> 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               <fct> male, female, female, female, male, female, male, fe…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n```\n:::\n:::\n\n\nIn this post we will use four variables from penguins data; which are bill_length_mm, bill_depth_mm, flipper_length_mm and body_mass_g. Use the `select()` function of **dplyr** package [@dplyr] to select the variables of interest.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin.data = penguin.data %>% \n  select(flipper_length_mm, \n         bill_length_mm, \n         bill_depth_mm,\n         body_mass_g)\n\npenguin.data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 333 × 4\n   flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n               <int>          <dbl>         <dbl>       <int>\n 1               181           39.1          18.7        3750\n 2               186           39.5          17.4        3800\n 3               195           40.3          18          3250\n 4               193           36.7          19.3        3450\n 5               190           39.3          20.6        3650\n 6               181           38.9          17.8        3625\n 7               195           39.2          19.6        4675\n 8               182           41.1          17.6        3200\n 9               191           38.6          21.2        3800\n10               198           34.6          21.1        4400\n# ℹ 323 more rows\n```\n:::\n:::\n\n\nBefore we apply ML regression algorithms, first we will look on the distribution of the response variable (body_mass_g). Our response variable ranges from around 2700 to 6700 grams with the median value at around 4050 grams [@fig-density]. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin.data %>% \n  mutate(med.mass = median(body_mass_g, na.rm = T)) %>% #median value = 4050\n  ggplot(aes(x = body_mass_g))+\n  geom_density(fill = \"cyan3\", alpha = 0.3)+\n  geom_vline(xintercept =  4050, color = \"red\", linetype = \"dashed\")+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![The distribution of the body mass of Penguin species. The vertical dashed red line indicate the median body mass](index_files/figure-html/fig-density-1.png){#fig-density width=384}\n:::\n:::\n\n\n\n\n## Data Spliting\n\nRegression algorithm as one of the supervised learning, needs two data types; the **training** and **testing** data set. The `penguin.data` will be split into two groups; training and testing dataset. The training dataset will have a proportion of 70% and the testing data will carry 30% of the total data. In total, our dataset has 333 observations in which 233 samples will be used to train the model while in validation of the accuracy of the model, we will use the testing set with 100 samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nsplit.penguin = penguin.data %>% \n  initial_split(prop = 0.7)\n\nsplit.penguin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<233/100/333>\n```\n:::\n:::\n\n\nThe training data below with 233 samples will be used to training the model;\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain.set = split.penguin %>% \n  training()\n\ntrain.set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 233 × 4\n   flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n               <int>          <dbl>         <dbl>       <int>\n 1               230           59.6          17          6050\n 2               184           34.4          18.4        3325\n 3               215           45.2          15.8        5300\n 4               210           49            19.5        3950\n 5               202           41.4          18.5        3875\n 6               203           51            18.8        4100\n 7               212           44.9          13.8        4750\n 8               225           51.1          16.5        5250\n 9               210           50.8          19          4100\n10               211           45.4          14.6        4800\n# ℹ 223 more rows\n```\n:::\n:::\n\n\nThe testing data with 100 observations will be used to test the accuracy of the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest.set = split.penguin %>% \n  testing()\n\ntest.set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 4\n   flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n               <int>          <dbl>         <dbl>       <int>\n 1               186           39.5          17.4        3800\n 2               195           40.3          18          3250\n 3               195           38.7          19          3450\n 4               194           46            21.5        4200\n 5               189           35.9          19.2        3800\n 6               185           38.2          18.1        3950\n 7               188           39.5          17.8        3300\n 8               186           36            18.5        3100\n 9               191           42.3          21.2        4150\n10               186           39.6          17.7        3500\n# ℹ 90 more rows\n```\n:::\n:::\n\n\n## Model specification\n\nIn order to perform the predictions on the body weight of the penguin species, we will use two different regression model types; The **Random Forest** and the **Linear Regression** model. The regression algorithm is applied when the response variable is numeric. We will look at one model after another and then at the end we will compare the best model among the two.\n\n### Random forest model\n\nFirst, we will specify the model type as random forest using the `rand_forest()` function of the **parsnip** package [@parsnip22]. The set engine **ranger** and the mode **regression** will be used. \n\nFinally, the model will be fitted with the train data set using the `fit()` function of the **parsnip** package [@parsnip22] with the response variable as **body_mass_g**.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.rf = rand_forest() %>% \n  set_engine(engine = \"ranger\") %>% \n  set_mode(mode = \"regression\") %>% \n  fit(body_mass_g~., data = train.set)\n\nmod.rf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      233 \nNumber of independent variables:  3 \nMtry:                             1 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       119311.9 \nR squared (OOB):                  0.8177082 \n```\n:::\n:::\n\n\nThe summary of the fitted model shows the R-squared value is 0.80. This shows that 80% of our data fits better to the model.\n\n#### Model Validation\n\nThen, we will evaluate the model's performance on the testing data using the `predict()` function of **stats** package [@r]. The model have achieved predictions of body weight of the penguins using the test data set which was not used in data training. The values of the predictions are within the distribution ranges of the response variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred.rf = mod.rf %>% \n  predict(test.set) %>% \n  bind_cols(test.set)\n\npred.rf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 5\n   .pred flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n   <dbl>             <int>          <dbl>         <dbl>       <int>\n 1 3583.               186           39.5          17.4        3800\n 2 3833.               195           40.3          18          3250\n 3 3931.               195           38.7          19          3450\n 4 3980.               194           46            21.5        4200\n 5 3709.               189           35.9          19.2        3800\n 6 3693.               185           38.2          18.1        3950\n 7 3587.               188           39.5          17.8        3300\n 8 3509.               186           36            18.5        3100\n 9 4040.               191           42.3          21.2        4150\n10 3595.               186           39.6          17.7        3500\n# ℹ 90 more rows\n```\n:::\n:::\n\n\n#### Accuracy testing\n\nAfter validating the model performance, then we need to test its accuracy using the `metrics()` function of **yardstick** package [@yardstick22]. The data used will be the output of the trained model (*pred.rf*), the **truth** variable will be the actual values of the body mass and the estimate variable will be the the predicted values of the body mass. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbb = pred.rf %>% \n  metrics(truth = body_mass_g,\n          estimate = .pred)\n\nbb\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard     343.   \n2 rsq     standard       0.815\n3 mae     standard     279.   \n```\n:::\n:::\n\n\nThe accuracy result shows the R-squared value of 0.81 meaning that our model is accurate for more than 80%. Other performance indicators like **Root Mean Squared Error (RMSE)** and the **Mean Absolute Error (MAE)** are also low. The RMSE measures the average difference between values predicted by a model and the actual values while the MAE measures the average absolute error between actual and predicted values.\n\n#### Model predictions on the new data\n\nFinally, we will use the tuned model to make predictions on new data. First we will create the new dataset using the `rnorm()` function of R software [@r]. We will name our new data as **data.new**.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.new = tibble(bill_depth_mm = rnorm(n = 10,\n                                  mean = 15, \n                                  sd = 3),\n            bill_length_mm = rnorm(n = 10,\n                                   mean = 50, \n                                   sd = 10),\n            flipper_length_mm = rnorm(n = 10,\n                                      mean = 195,\n                                      sd = 50))\n\nmod.rf %>% \n  predict(data.new) %>% \n  bind_cols(data.new) %>% \n  mutate(.pred = as.integer(.pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 4\n   .pred bill_depth_mm bill_length_mm flipper_length_mm\n   <int>         <dbl>          <dbl>             <dbl>\n 1  5417          14.0           57.9              263.\n 2  3305          14.1           29.0              166.\n 3  4022          15.9           49.6              155.\n 4  3818          20.5           46.0              168.\n 5  5266          12.1           48.9              235.\n 6  4179          21.6           68.0              160.\n 7  3743          14.4           41.9              131.\n 8  5599          17.9           69.0              314.\n 9  4192          12.4           57.1              140.\n10  4745          13.5           57.4              205.\n```\n:::\n:::\n\n\nThat's great!!\n\nOur model has predicts the body mass for the penguin species for the new data. \n\n### Linear regression model\n\nWhen using the linear regression model we will first specify the model type as Linear Regression using the `linear_reg()` function of the **parsnip** package [@parsnip22]. The set engine **lm** and the mode **regression** will be used. \n\nFinally, the model will be fitted with the train data set using the `fit()` function of the **parsnip** package [@parsnip22] with the response variable as **body_mass_g**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.lm = linear_reg() %>% \n  set_engine(engine = \"lm\") %>% \n  set_mode(mode = \"regression\") %>% \n  fit(body_mass_g~., data = train.set %>% select(-flipper_length_mm))\n\nmod.lm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = body_mass_g ~ ., data = data)\n\nCoefficients:\n   (Intercept)  bill_length_mm   bill_depth_mm  \n       2704.67           82.83         -123.51  \n```\n:::\n:::\n\n\n#### Model Validation\n\nThen, we will evaluate the model's performance on the testing data using the `predict()` function of **stats** package [@r]. Our model has predicted for the body weight values of the penguins.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred.lm = mod.lm %>% \n  predict(test.set) %>% \n  bind_cols(test.set)\n\npred.lm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 5\n   .pred flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n   <dbl>             <int>          <dbl>         <dbl>       <int>\n 1 3827.               186           39.5          17.4        3800\n 2 3819.               195           40.3          18          3250\n 3 3563.               195           38.7          19          3450\n 4 3859.               194           46            21.5        4200\n 5 3307.               189           35.9          19.2        3800\n 6 3633.               185           38.2          18.1        3950\n 7 3778.               188           39.5          17.8        3300\n 8 3401.               186           36            18.5        3100\n 9 3590.               191           42.3          21.2        4150\n10 3798.               186           39.6          17.7        3500\n# ℹ 90 more rows\n```\n:::\n:::\n\n\n#### Accuracy testing\n\nThe model accuracy will be tested using the `metrics()` function of **yardstick** package [@yardstick22]. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncc = pred.lm %>% \n  metrics(truth = body_mass_g,\n          estimate = .pred)\n\ncc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard     616.   \n2 rsq     standard       0.411\n3 mae     standard     506.   \n```\n:::\n:::\n\n\n\nThe accuracy result shows the R-square is 0.41, RMSE is 616.14 and the MAE value is 505.88. \n\n## Comparison between the two models\n\nWe have seen the performance of each model. We have to choose which model is more accurate and performs better than the other. I will join the two accuracy test results; the one from the random forest and the other from the linear regression model (@tbl-model-best). \n\n\n::: {#tbl-model-best .cell tbl-cap='The table showing the model performance indicators'}\n\n```{.r .cell-code}\npred.rf %>% \n  metrics(truth = body_mass_g,\n          estimate = .pred) %>% \n  pivot_wider(names_from = .metric, \n              values_from = .estimate) %>% \n  mutate(model = \"Random Forest\") %>% \n  select(Model = 5, Rsquare=3, RMSE=2, MAE=4) %>% \n  bind_rows(\n    \npred.lm %>% \n  metrics(truth = body_mass_g,\n          estimate = .pred)  %>% \n  pivot_wider(names_from = .metric, \n              values_from = .estimate) %>% \n  mutate(model = \"Linear Regression\") %>% \n  select(Model = 5, Rsquare=3, RMSE=2, MAE=4)\n) %>% \n  mutate(across(is.numeric, round, 2))  %>% \n  gt::gt()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(is.numeric, round, 2)`.\nCaused by warning:\n! Use of bare predicate functions was deprecated in tidyselect 1.1.0.\nℹ Please use wrap predicates in `where()` instead.\n  # Was:\n  data %>% select(is.numeric)\n\n  # Now:\n  data %>% select(where(is.numeric))\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n```\n:::\n\n::: {.cell-output-display}\n```{=html}\n<div id=\"idbvzlikgm\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>html {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#idbvzlikgm .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#idbvzlikgm .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#idbvzlikgm .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#idbvzlikgm .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#idbvzlikgm .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#idbvzlikgm .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#idbvzlikgm .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#idbvzlikgm .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#idbvzlikgm .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#idbvzlikgm .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#idbvzlikgm .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#idbvzlikgm .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#idbvzlikgm .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#idbvzlikgm .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#idbvzlikgm .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#idbvzlikgm .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#idbvzlikgm .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#idbvzlikgm .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#idbvzlikgm .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#idbvzlikgm .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#idbvzlikgm .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#idbvzlikgm .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#idbvzlikgm .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#idbvzlikgm .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#idbvzlikgm .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#idbvzlikgm .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#idbvzlikgm .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#idbvzlikgm .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#idbvzlikgm .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#idbvzlikgm .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-left: 4px;\n  padding-right: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#idbvzlikgm .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#idbvzlikgm .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#idbvzlikgm .gt_left {\n  text-align: left;\n}\n\n#idbvzlikgm .gt_center {\n  text-align: center;\n}\n\n#idbvzlikgm .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#idbvzlikgm .gt_font_normal {\n  font-weight: normal;\n}\n\n#idbvzlikgm .gt_font_bold {\n  font-weight: bold;\n}\n\n#idbvzlikgm .gt_font_italic {\n  font-style: italic;\n}\n\n#idbvzlikgm .gt_super {\n  font-size: 65%;\n}\n\n#idbvzlikgm .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 75%;\n  vertical-align: 0.4em;\n}\n\n#idbvzlikgm .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#idbvzlikgm .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#idbvzlikgm .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#idbvzlikgm .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#idbvzlikgm .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#idbvzlikgm .gt_indent_5 {\n  text-indent: 25px;\n}\n</style>\n<table class=\"gt_table\">\n  \n  <thead class=\"gt_col_headings\">\n    <tr>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Model\">Model</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Rsquare\">Rsquare</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"RMSE\">RMSE</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"MAE\">MAE</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"Model\" class=\"gt_row gt_left\">Random Forest</td>\n<td headers=\"Rsquare\" class=\"gt_row gt_right\">0.81</td>\n<td headers=\"RMSE\" class=\"gt_row gt_right\">343.03</td>\n<td headers=\"MAE\" class=\"gt_row gt_right\">279.11</td></tr>\n    <tr><td headers=\"Model\" class=\"gt_row gt_left\">Linear Regression</td>\n<td headers=\"Rsquare\" class=\"gt_row gt_right\">0.41</td>\n<td headers=\"RMSE\" class=\"gt_row gt_right\">616.14</td>\n<td headers=\"MAE\" class=\"gt_row gt_right\">505.88</td></tr>\n  </tbody>\n  \n  \n</table>\n</div>\n```\n:::\n:::\n\n\nThe table shows that the Random Forest is the best than the Linear Regression Model!! This is because it has higher R-squared value with low RMSE and MAE values (@tbl-model-best).\n\n\n\n\n\n\n## Summary\n\n\nAll in all, the success of machine learning and artificial intelligence depend on the **quality of the data**, the **complexity of the problem**, and the **choice of the appropriate algorithms** and techniques [@han11; @witten05].\n\nThe quality of the data is one of the most important factors for the success of machine learning. The data should be accurate, complete, and representative of the problem domain [@han11; @witten05]. In addition, the data should be properly labeled and preprocessed to ensure that the machine learning algorithms can effectively learn from it.\n\nThe complexity of the problem is also a key factor in determining the success of machine learning [@gomez14]. Some problems are inherently more complex than others, and require more sophisticated algorithms and techniques to solve. For example, image recognition and natural language processing are typically more complex than simple regression problems.\n\nFinally, the choice of the appropriate algorithms and techniques is critical for the success of machine learning [@sarker21; @gomez14]. Different algorithms and techniques are suited for different types of problems, and the choice of the appropriate one will depend on the specific problem and the available data. Additionally, the parameters and hyperparameters of the algorithms need to be properly tuned to ensure that the models are optimized for the problem at hand.\n\n::: callout-note\n\nDon't miss out our next post in this blog!!!\n\n:::\n## Consultated references\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}