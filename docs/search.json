[
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "How to create GitHub site as a web page\n\n\n\n\n\n\n\n\n\nJun 16, 2024\n\n\n\n\n\n\n\n\nExtracting values from raster image using terra package in R\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\n\n\n\n\n\n\nCropping raster image using terra and tidyterra packages in R\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n\n\nHow to Remove a Large Staged File from Git\n\n\n\n\n\n\n\n\n\nMar 20, 2024\n\n\n\n\n\n\n\n\nThe Surpass Global Ocean Temperature Record: Implications for Ocean Productivity in Tanzania\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\n\n\n\n\n\n\nMachine Learning (ML) using Regression Algorithm in R\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\n\n\n\n\n\n\nMachine Learning (ML) using classification Algorithm in R\n\n\n\n\n\n\n\n\n\nMar 8, 2023\n\n\n\n\n\n\n\n\nDeploying and publishing to GitHub\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\n\n\n\n\n\n\nText analytics and alluvial plots in R\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\nText analytics and word cloud in R and ggplot2\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\nThe Family Affairs: A Look into the Importance of Family\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\n\n\n\n\n\n\nThe Memorable Summer Time\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Nyamisi Peter",
    "section": "",
    "text": "How to create GitHub site as a web page\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2024\n\n\nNyamisi Peter\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nExtracting values from raster image using terra package in R\n\n\n\n\n\n\n\nterra\n\n\nvisualization\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nNyamisi Peter\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nCropping raster image using terra and tidyterra packages in R\n\n\n\n\n\n\n\nterra\n\n\nvisualization\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nNyamisi Peter\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nHow to Remove a Large Staged File from Git\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2024\n\n\nNyamisi Peter\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nThe Surpass Global Ocean Temperature Record: Implications for Ocean Productivity in Tanzania\n\n\n\n\n\n\n\nvisualization\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\nNyamisi Peter\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning (ML) using Regression Algorithm in R\n\n\n\n\n\n\n\ncode\n\n\nML & AI\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nNyamisi Peter\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning (ML) using classification Algorithm in R\n\n\n\n\n\n\n\ncode\n\n\nML & AI\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2023\n\n\nNyamisi Peter\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nDeploying and publishing to GitHub\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nNyamisi Peter\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nText analytics and alluvial plots in R\n\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nNyamisi Peter\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nText analytics and word cloud in R and ggplot2\n\n\n\n\n\n\n\nvisualization\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nNyamisi Peter\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nThe Family Affairs: A Look into the Importance of Family\n\n\n\n\n\n\n\naffairs\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nNyamisi Peter\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nThe Memorable Summer Time\n\n\n\n\n\n\n\naffairs\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\nGrace Semba\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nyamisi Peter",
    "section": "",
    "text": "Hi, I’m Nyamisi Peter, an Assistant Lecturer at the University of Dar es Salaam. I served the same position at the University of Dodoma from 2015 to 2022 before I joined the University of Dar es Salaam in 2022.\n\n\nI am a data scientist and marine biologist focusing on ocean productivity. I am doing programming and coding using different programming languages like R, Python, html, Quarto, css and several other programming languages. I like working with data especially data processing and manipulation, data analysis and visualization using R software.\nI graduated my first degree from University of Dar es Salaam with a degree in Aquatic Environmental Sciences and Conservation in 2009. I then joined postgraduate studies at the Institute of Marine Sciences (IMS), Zanzibar, where I earned Master of Science in Marine Sciences degree in 2013. Currently, I am doing my PhD studies in Aquatic Sciences focusing on the effects of global warming on ocean productivity at the University of Dar es Salaam.\nNyamisi Peter, she is a wife, married to Masumbuko Semba. She is also a mother of three kids: Grace, Daniel and Ethan."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Nyamisi Peter",
    "section": "References",
    "text": "References\n\nSemba, M., Kimirei, I., Kyewalyanga, M., Peter, N., Brendonck, L., & Somers, B. (2016). The decline in phytoplankton biomass and prawn catches in the Rufiji-Mafia Channel, Tanzania. Western Indian Ocean Journal of Marine Science, 15(1), 15-29.\nPeter, N., Semba, M., Lugomela, C., & Kyewalyanga, M. S. (2018). The influence of physical-chemical variables on the spatial and seasonal variation of Chlorophyll-a in coastal waters of Unguja, Zanzibar, Tanzania. Western Indian Ocean Journal of Marine Science, 17(2), 25-34.\nKyewalyanga, M. S., Peter, N., Semba, M., & Mahongo, S. B. (2020). Coastal upwelling and seasonal variation in phytoplankton biomass in the Pemba Channel. Western Indian Ocean Journal of Marine Science, (1/2020), 19-32.\nPeter, N., Semba, M., Lugomela, C., & Kyewalyanga, M. (2021). Seasonal variability of vertical patterns in chlorophyll-a fluorescence in the coastal waters off Kimbiji, Tanzania. Western Indian Ocean Journal of Marine Science, 20(1), 21-33."
  },
  {
    "objectID": "posts/alluvial/index.html",
    "href": "posts/alluvial/index.html",
    "title": "Text analytics and alluvial plots in R",
    "section": "",
    "text": "In the previous post, we covered how to present textual information in wordcloud. In this post, we are going to extend what we have learned previous with a new visualization technique in town—alluvial plots. An alluvial diagram or alluvial plot is a type of visualization that shows changes in flow over time. Alluvial plots are commonly used to show how information flows from one source and feed to another source or destination. Several packages are used to create alluvial plots, but in this post we are going to learn how to use ggalluvial package (Brunson, 2020) to make alluvial plots in R (R Core Team, 2022). If the package ks installed in your computer, you can simply download and install it from CRAN as;\n\ninstall.packages(\"alluvial\")\n\nBefore we proceed, we need to load some packages, whose function we are going to use throughout this post. These packages are highlighted in the chunk below;\n\nrequire(tidyverse)\nrequire(readxl)\nrequire(tm)           # for text mining\nrequire(SnowballC)    # for text stemming\nrequire(wordcloud)    # word-cloud generator\nrequire(RColorBrewer) # color palettes\nrequire(wordcloud2)\nrequire(ggalluvial)"
  },
  {
    "objectID": "posts/github-deploy-publish/index.html",
    "href": "posts/github-deploy-publish/index.html",
    "title": "Deploying and publishing to GitHub",
    "section": "",
    "text": "GitHub is a web-based platform that provides version control and collaborative software development services. It is primarily used for source code management and version control. GitHub also provides features such as bug tracking, task management, and project wikis.\nGitHub allows developers to collaborate on projects with other developers around the world. Users can create and manage their own repositories, which are collections of files and folders that are tracked by Git, a distributed version control system. Developers can then use Git to make changes to the codebase, track those changes over time, and collaborate with others on the same codebase.\nGitHub also provides a social networking aspect, allowing users to follow other developers and organizations, explore popular repositories, and contribute to open source projects. It is widely used in the software development community, with millions of users and tens of millions of repositories.\nIn addition to its core features, GitHub also offers additional services such as GitHub Pages, which allows users to host static websites on the platform, and GitHub Actions, which provides continuous integration and continuous deployment services.\n\n\nDeploying to GitHub has several benefits, including:\n\nVersion control: GitHub provides an easy-to-use platform for version control, which allows you to track changes to your code over time and collaborate with other developers.\nFree hosting: GitHub offers free hosting for static websites through its GitHub Pages service. This means you can host your website on GitHub without having to pay for hosting services.\nAccessibility: GitHub is widely used by developers around the world, which means your code and website are accessible to a large audience. It also makes it easy for others to contribute to your project.\nShowcase your work: GitHub provides a platform to showcase your work to potential employers or clients. You can also use GitHub to build a portfolio of your work, which can be a valuable asset in your career.\nContinuous integration: GitHub supports continuous integration, which allows you to automatically build and deploy your code whenever changes are made. This can save time and effort in the deployment process.\n\nGenerally, deploying to GitHub can help streamline the development process, provide a platform to showcase your work, and offer free hosting for your projects.\n\n\n\n\n\n\nSteps in GitHub deployment\n\n\nDeploying to GitHub typically involves the following steps:\nFirst, create a GitHub account: If you don’t already have one. You may create an account by opening GitHub page. Click the Sign up button at the top right corner of the page. This will navigate to the sign up window at GitHubSignUp. Fill in your particulars including user name and password.\n\n\n\nSign up to GitHub\n\n\nAfter you have already created an account, create a new repository in GitHub from which you will deploy your work: Navigate to the GitHub homepage and click the New Repository button in the top-right corner of the page. This will open a new window. Give your repository a name; I have named my repository as github-deployment. Choose whether it should be public or private, and add a description if you like and click create repository\n\n\n\nCreating a new repository in GitHub\n\n\nInitialize Git: In your local development environment (Local Host) by opening the GitBash. Navigate to the directory where your project is located in your local host using cd function of the GitBash. Run the command git init to initialize Git. Check the status of your work by running git status in GitBash. It is adviced to check the status of your work after every step before proceeding to the new step to crosscheck if there is no any error or mistake made in the previous step\nAdd files to the repository: Use the git add command to add the files you want to include in your repository. You can use the command git add . (add .(period)) to add all the files in your current directory. Check for status of your work\nCommit changes: Use the git commit command to commit your changes to the repository. Remember to include a descriptive commit message to explain what changes you made. Check for the status of your work\nAdd the remote repository: Use the command git remote add origin https://github.com/your-username/your-repository.git to add the remote repository as the origin. Check for status of your work\nIf the working tree is clean, Push changes to the remote repository: Use the command git push -u origin main to push your changes to the remote repository. The -u flag sets the upstream branch to main, which means you’ll only need to run git push in the future to push changes.\nVerify deployment: Once you’ve pushed your changes, navigate to your repository on GitHub and verify that the changes have been deployed successfully.\nThat’s it! Your project is now deployed to GitHub and your client may access it from the cloud.\nIf you made changes to your project after first commit, then you will only run three steps to make those changes to your GitHub repository; add, commit and push\nAdd all your files in the working directory using git add .\nCommit changes to your repository using git commit -m “Add commit message”\npush your work using git push\nNavigate to your repository on GitHub and verify that the changes have been deployed successfully.\nIf the changes made to your GitHub repository are successful, you may wish to publish your repository and make it accessible to the public as a web page"
  },
  {
    "objectID": "posts/machine-learning-classification/index.html",
    "href": "posts/machine-learning-classification/index.html",
    "title": "Machine Learning (ML) using classification Algorithm in R",
    "section": "",
    "text": "Technology is becoming more important in our daily lives in every second. In order to keep up with the pace of these technological changes, scientists are more heavily learning different algorithms to make things easier so as to meet consumer’s demand. These technologies are commonly associated with artificial intelligence, machine learning, deep learning, and neural networks.\nMachine learning (ML) and artificial intelligence (AI) are closely related concepts that are often used interchangeably, but they are not the same thing.\nArtificial intelligence refers to the ability of machines to mimic human cognitive functions such as learning, reasoning, and problem-solving IBM. AI encompasses a wide range of techniques and approaches, including rule-based systems, expert systems, and machine learning. It is used to predict, automate, and optimize tasks that humans have historically done, such as speech and facial recognition, decision making, and translation.\nMachine learning is a specific type of AI that involves the development of algorithms that can learn from data and make predictions or decisions based on that data (Mitchell et al., 2007). In other words, machine learning algorithms are designed to learn patterns and relationships in data without being explicitly programmed to do so.\n\n\n\nMachine learning\n\n\n\n\nThere are several types of machine learning algorithms, including supervised learning, unsupervised learning, and reinforcement learning. Supervised learning involves using labeled (predictor) training data to train a model to make predictions on new, unseen data. Unsupervised learning involves finding patterns and relationships in data without the use of labeled training data. Reinforcement learning involves training a model to make decisions based on feedback in the form of rewards or punishments.\nClassification and regression are two of the most common types of supervised learning algorithms in machine learning and artificial intelligence. Classification is a type of supervised learning algorithm used for predicting discrete or categorical outcomes. It involves mapping input variables to discrete output categories or labels. The objective of classification is to build a model that accurately assigns new data points to the correct class or label.\nOn the other hand, regression is a type of supervised learning algorithm used for predicting continuous or numeric outcomes. It involves mapping input variables to continuous output values. The objective of regression is to build a model that accurately predicts the value of the dependent variable based on the values of the independent variables.\n\n\n\nDifference between classification and regression\n\n\nIn this tutorial we are going to deal with classification algorithm in predicting the type of penguin species flipper length, bill dimensions and sex. We will use penguins data from the palmerpenguins package (Horst et al., 2020). It includes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\nFirst, we will load the packages which we are going to use in this tutorial; I will use require() function, but you may also use library() function depending on your preferences.\n\nrequire(tidyverse)\nrequire(tidymodels)\nrequire(palmerpenguins)\nrequire(ranger)\nrequire(patchwork)\n\n\n\n\nAfter loading the packages, we will load the penguins data (Horst et al., 2020) and remove all the missing values in the dataset and equate them as penguin.data. The dataset consist of 333 rows and 8 columns\n\npenguin.data = palmerpenguins::penguins %&gt;% \n  drop_na()\n\npenguin.data\n\n# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 323 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThen, we will look on the internal structure of the dataset using glimpse() function of dplyr package. Our dataset consist of 8 variables (columns); 3 are factors data (species, island and sex), 2 numeric or double data (bill_length_mm, bill_depth_mm) and 3 integers (flipper_length_mm, body_mass_g, year). The variable species have 3 different levels which are Adelie, Gentoo and Chinstrap\n\nglimpse(penguin.data)\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nIn this tutorial we will use only three variables from penguins data; which are species, bill_length_mm, bill_depth_mm. We will run the select() function of dplyr package (Wickham et al., 2022) to select our variables of interest\n\npenguin.data = penguin.data %&gt;% \n  select(species, \n         bill_length_mm, \n         bill_depth_mm)\n\nBefore we apply ML algorithms, first we will crosscheck whether the predictor variable (in this case, species), have distinct features (like size) which will help in providing the more accurate output during predictions. When there is interconnection or inter-relation between response variables, some confusion might arise during predictions.\nWe will use scatter plot between bill_length_mm and bill_depth_mm to see the distribution of each species in the dataset. There is a distinct differences between the size of the three penguins species (Figure 1). Each species has its size range; therefore, the dataset fit best in our analysis.\n\npenguin.data %&gt;% \n  ggplot(aes(y = bill_length_mm, \n             x = bill_depth_mm, \n             color = species)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\nFigure 1: Scatter plot showing the distribution of Adelie, Gentoo and Chinstrap species\n\n\n\n\n\n\n\nAs explained earlier, supervised learning involves training a dataset before you have your predictions. Since our output is categorical (prediction of the species type), then we will use classification algorithm in our analysis.\n\n\nClassification algorithm as one of the supervised learning, needs two data types; the training and testing data set. Our penguin.data will be split into these two groups. The training dataset will have a proportion of 70% and the remaining 30% will be the test dataset. In total, our dataset has 333 observations in which 233 samples will be used to train our model while in testing the accuracy of the model 100 samples will used.\n\nsplit.penguin = penguin.data %&gt;% \n  initial_split(prop = 0.7)\n\nsplit.penguin\n\n&lt;Training/Testing/Total&gt;\n&lt;233/100/333&gt;\n\n\nThe training data below with 233 samples will be used for training the model;\n\ntrain.set = split.penguin %&gt;% \n  training()\n\ntrain.set\n\n# A tibble: 233 × 3\n   species   bill_length_mm bill_depth_mm\n   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie              36.2          17.2\n 2 Gentoo              48.7          15.1\n 3 Chinstrap           49.8          17.3\n 4 Chinstrap           54.2          20.8\n 5 Gentoo              44.9          13.3\n 6 Chinstrap           50.7          19.7\n 7 Adelie              38.6          17  \n 8 Gentoo              46.2          14.9\n 9 Gentoo              50.1          15  \n10 Adelie              39.8          19.1\n# ℹ 223 more rows\n\n\nThe testing data with 100 observations will be used to test the accuracy of the model\n\ntest.set = split.penguin %&gt;% \n  testing()\n\ntest.set\n\n# A tibble: 100 × 3\n   species bill_length_mm bill_depth_mm\n   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie            40.3          18  \n 2 Adelie            38.9          17.8\n 3 Adelie            38.6          21.2\n 4 Adelie            42.5          20.7\n 5 Adelie            37.8          18.3\n 6 Adelie            37.7          18.7\n 7 Adelie            35.9          19.2\n 8 Adelie            38.8          17.2\n 9 Adelie            35.3          18.9\n10 Adelie            40.5          17.9\n# ℹ 90 more rows\n\n\n\n\nUse the train.set data to train the model. Run the rand_forest() of the parsnip package (Kuhn and Vaughan, 2022), set engine as ranger and the mode as classification. Fit the train data set with the response be the species. The summary of the training model will be shown with the predictions error of 0.02 equivalent to 2%.\n\nmod = rand_forest() %&gt;% \n  set_engine(engine = \"ranger\") %&gt;% \n  set_mode(mode = \"classification\") %&gt;% \n  fit(species~., data = train.set)\n\nmod\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      233 \nNumber of independent variables:  2 \nMtry:                             1 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.03958214 \n\n\n\n\n\nThen, the test.set data will be used in predictions of the type of the species provided we have the bill length and bill depth. The predicted class and the test dataset will be binded together to see if there is a match or mismatch of the predicted versus the actual species type.\n\nrf.pred = mod %&gt;% \n  predict(test.set) %&gt;% \n  bind_cols(test.set)\n\nrf.pred\n\n# A tibble: 100 × 4\n   .pred_class species bill_length_mm bill_depth_mm\n   &lt;fct&gt;       &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie      Adelie            40.3          18  \n 2 Adelie      Adelie            38.9          17.8\n 3 Adelie      Adelie            38.6          21.2\n 4 Adelie      Adelie            42.5          20.7\n 5 Adelie      Adelie            37.8          18.3\n 6 Adelie      Adelie            37.7          18.7\n 7 Adelie      Adelie            35.9          19.2\n 8 Adelie      Adelie            38.8          17.2\n 9 Adelie      Adelie            35.3          18.9\n10 Adelie      Adelie            40.5          17.9\n# ℹ 90 more rows\n\n\n\n\n\nWe have already predicted for the results, then we need to test for the accuracy of the model. The model is considered accurate when its accuracy value is higher or equal to 80%. The obtained accuracy of our model is 0.98 (98%) and the kap 96%. Our model can then be used to predict the type of species provided you have the bill length and the bill depth of the penguins.\n\nrf.pred %&gt;% \n  metrics(truth = species,\n          estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.94 \n2 kap      multiclass     0.905\n\n\nNow, we need to see the accuracy for each predicted species versus the actual type. We will use conf_mat() of the yardstick package (Kuhn et al., 2022). The result indicate that 43 Adelie were correctly predicted while 1 Chinstrap was incorrectly predicted as Adelie. While 16 Chinstrap were correctly predicted, 1 of this species was wrongly predicted as Gentoo. On the other hand, all 39 Gentoo were correctly predicted.\n\nrf.pred %&gt;% \n  conf_mat(truth = species, \n           estimate = .pred_class)\n\n           Truth\nPrediction  Adelie Chinstrap Gentoo\n  Adelie        46         1      0\n  Chinstrap      2        17      2\n  Gentoo         0         1     31\n\n\nThen, we need to calculate the probability of each observation be accurate for every species. We will use predict() function of *stats package of R (R Core Team, 2022).\n\nrf.preda = mod %&gt;% \n  predict(test.set, \n          type = \"prob\") %&gt;% \n  bind_cols(test.set)\n\nrf.preda\n\n# A tibble: 100 × 6\n   .pred_Adelie .pred_Chinstrap .pred_Gentoo species bill_length_mm\n          &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;            &lt;dbl&gt;\n 1        1             0           0        Adelie            40.3\n 2        0.997         0.00313     0        Adelie            38.9\n 3        0.996         0.00396     0        Adelie            38.6\n 4        0.796         0.182       0.0223   Adelie            42.5\n 5        0.999         0.00075     0        Adelie            37.8\n 6        0.989         0.0107      0        Adelie            37.7\n 7        0.998         0.00156     0        Adelie            35.9\n 8        0.996         0.00327     0.000286 Adelie            38.8\n 9        0.997         0.003       0        Adelie            35.3\n10        1             0           0        Adelie            40.5\n# ℹ 90 more rows\n# ℹ 1 more variable: bill_depth_mm &lt;dbl&gt;\n\n\n\n\n\nThe probability results will then be used to plot the roc curve (Figure 2). The roc_curve() function of yardstick package (Kuhn et al., 2022) will be used supplied with species, .pred_Adelie, .pred_Chinstrap, and .pred_Gentoo. The autoplot() function of the workflowsets package (Kuhn and Couch, 2022) will then be applied to create the curve. The curve shows that Adelie species fitted better to the model than other the Gentoo and Chinstrap species (Figure 2).\n\nrf.preda %&gt;% \n  roc_curve(species, \n            .pred_Adelie, \n            .pred_Chinstrap, \n            .pred_Gentoo) %&gt;% \n  autoplot()+\n  ggpubr::theme_pubclean()+\n  theme(strip.background = element_blank())\n\n\n\n\nFigure 2: The ROC curve of Adelie, Chinstrap and Gentoo species\n\n\n\n\n\n\n\nIn order to validate whether the model works to other newly collected data, we will validate it using the created data consists of bill length and depth. The data will be applied to the model and test if the prediction of the type of species will be done. The new created data will be named as new.penguin containing three observations;\n\nnew.penguin = tibble(bill_length_mm = c(35,47.5, 70),\n                     bill_depth_mm = c(13,17, 18))\n\nnew.penguin\n\n# A tibble: 3 × 2\n  bill_length_mm bill_depth_mm\n           &lt;dbl&gt;         &lt;dbl&gt;\n1           35              13\n2           47.5            17\n3           70              18\n\n\nThen, the model will be applied and see if it will predict the type of species. We will use predict() function.\nHoolah!! the model give us the predicted species as Adelie and Chinstrap\n\nmod %&gt;% \n  predict(new.penguin)\n\n# A tibble: 3 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 Adelie     \n2 Chinstrap  \n3 Chinstrap  \n\n\nWe will also create another data named as aa with 10 observations using rnorm() function of stats package (R Core Team, 2022). We will also predict the type of species for these data using our model. Again the model works and give us predictions!!\n\naa = tibble(bill_depth_mm = rnorm(n = 10,\n                                  mean = 15, \n                                  sd = 3),\n            bill_length_mm = rnorm(n = 10,\n                                   mean = 50, \n                                   sd = 10))\n\nmod %&gt;% \n  predict(aa) %&gt;% \n  bind_cols(aa)\n\n# A tibble: 10 × 3\n   .pred_class bill_depth_mm bill_length_mm\n   &lt;fct&gt;               &lt;dbl&gt;          &lt;dbl&gt;\n 1 Adelie               14.3           30.4\n 2 Gentoo               13.9           62.6\n 3 Gentoo               12.9           71.0\n 4 Chinstrap            18.0           50.5\n 5 Gentoo               13.4           53.5\n 6 Chinstrap            18.7           49.9\n 7 Gentoo               14.9           46.5\n 8 Adelie               17.2           30.2\n 9 Gentoo               14.7           53.5\n10 Gentoo               10.8           48.7\n\n\n\n\n\n\n\nAll in all, the success of machine learning and artificial intelligence depend on the quality of the data, the complexity of the problem, and the choice of the appropriate algorithms and techniques (Jiawei Han and Pei, 2011; Witten et al., 2005).\nThe quality of the data is one of the most important factors for the success of machine learning. The data should be accurate, complete, and representative of the problem domain (Jiawei Han and Pei, 2011; Witten et al., 2005). In addition, the data should be properly labeled and preprocessed to ensure that the machine learning algorithms can effectively learn from it.\nThe complexity of the problem is also a key factor in determining the success of machine learning (Gomez-Cabrero et al., 2014). Some problems are inherently more complex than others, and require more sophisticated algorithms and techniques to solve. For example, image recognition and natural language processing are typically more complex than simple regression problems.\nFinally, the choice of the appropriate algorithms and techniques is critical for the success of machine learning (Gomez-Cabrero et al., 2014; Sarker, 2021). Different algorithms and techniques are suited for different types of problems, and the choice of the appropriate one will depend on the specific problem and the available data. Additionally, the parameters and hyperparameters of the algorithms need to be properly tuned to ensure that the models are optimized for the problem at hand.\n\n\n\n\n\n\nNote\n\n\n\nDon’t miss out our next tutorial on Machine Learning (ML) using regression Algorithm!!!"
  },
  {
    "objectID": "posts/machine-learning-regression/index.html",
    "href": "posts/machine-learning-regression/index.html",
    "title": "Machine Learning (ML) using Regression Algorithm in R",
    "section": "",
    "text": "In the previous post we saw how classification algorithm may be applied in machine learning (ML). In this post we are going to look the application of regression algorithm in machine learning. We will use penguins data from the palmerpenguins package (Horst et al., 2020). It includes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\nFirst, we will load the packages which we are going to use in this post; I will use require() function, but you may also use library() function depending on your preferences. The tidyverse is a collection of R packages that are designed to work together to make data manipulation and analysis easier and more efficient. Tidyverse is used as a workflow from data preprocessing to model fitting and evaluation. The packages in the Tidyverse, such as dplyr and ggplot2, will be used in data manipulation and plotting of graphs respectively.\nTidymodels, on the other hand, is a collection of R packages for modeling and machine learning tasks. Tidymodels includes packages such as rsample for the initial splitting and training of the data, parsnip for specifying and fitting models, and yardstick for evaluating model performance. The ranger package will be used in setting the engine in random forest model.\nThe data used will come from palmerpenguins package.\n\nrequire(tidyverse)\nrequire(tidymodels)\nrequire(ranger)\nrequire(palmerpenguins)\n\n\n\nAfter loading the packages, load the penguins data (Horst et al., 2020) and remove all the missing values in the dataset. Name the dataset as penguin.data. The dataset consist of 333 rows and 8 columns\n\npenguin.data = palmerpenguins::penguins %&gt;% \n  drop_na()\n\npenguin.data\n\n# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 323 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThen, observe the internal structure of the dataset using glimpse() function of dplyr package. The dataset consist of 8 variables (columns); 3 are factors data (species, island and sex), 2 numeric or double data (bill_length_mm, bill_depth_mm) and 3 integers (flipper_length_mm, body_mass_g, year).\n\nglimpse(penguin.data)\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nIn this post we will use four variables from penguins data; which are bill_length_mm, bill_depth_mm, flipper_length_mm and body_mass_g. Use the select() function of dplyr package (Wickham et al., 2022) to select the variables of interest.\n\npenguin.data = penguin.data %&gt;% \n  select(flipper_length_mm, \n         bill_length_mm, \n         bill_depth_mm,\n         body_mass_g)\n\npenguin.data\n\n# A tibble: 333 × 4\n   flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n               &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;\n 1               181           39.1          18.7        3750\n 2               186           39.5          17.4        3800\n 3               195           40.3          18          3250\n 4               193           36.7          19.3        3450\n 5               190           39.3          20.6        3650\n 6               181           38.9          17.8        3625\n 7               195           39.2          19.6        4675\n 8               182           41.1          17.6        3200\n 9               191           38.6          21.2        3800\n10               198           34.6          21.1        4400\n# ℹ 323 more rows\n\n\nBefore we apply ML regression algorithms, first we will look on the distribution of the response variable (body_mass_g). Our response variable ranges from around 2700 to 6700 grams with the median value at around 4050 grams Figure 1.\n\npenguin.data %&gt;% \n  mutate(med.mass = median(body_mass_g, na.rm = T)) %&gt;% #median value = 4050\n  ggplot(aes(x = body_mass_g))+\n  geom_density(fill = \"cyan3\", alpha = 0.3)+\n  geom_vline(xintercept =  4050, color = \"red\", linetype = \"dashed\")+\n  theme_bw()\n\n\n\n\nFigure 1: The distribution of the body mass of Penguin species. The vertical dashed red line indicate the median body mass\n\n\n\n\n\n\n\nRegression algorithm as one of the supervised learning, needs two data types; the training and testing data set. The penguin.data will be split into two groups; training and testing dataset. The training dataset will have a proportion of 70% and the testing data will carry 30% of the total data. In total, our dataset has 333 observations in which 233 samples will be used to train the model while in validation of the accuracy of the model, we will use the testing set with 100 samples.\n\nset.seed(123)\nsplit.penguin = penguin.data %&gt;% \n  initial_split(prop = 0.7)\n\nsplit.penguin\n\n&lt;Training/Testing/Total&gt;\n&lt;233/100/333&gt;\n\n\nThe training data below with 233 samples will be used to training the model;\n\ntrain.set = split.penguin %&gt;% \n  training()\n\ntrain.set\n\n# A tibble: 233 × 4\n   flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n               &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;\n 1               230           59.6          17          6050\n 2               184           34.4          18.4        3325\n 3               215           45.2          15.8        5300\n 4               210           49            19.5        3950\n 5               202           41.4          18.5        3875\n 6               203           51            18.8        4100\n 7               212           44.9          13.8        4750\n 8               225           51.1          16.5        5250\n 9               210           50.8          19          4100\n10               211           45.4          14.6        4800\n# ℹ 223 more rows\n\n\nThe testing data with 100 observations will be used to test the accuracy of the model\n\ntest.set = split.penguin %&gt;% \n  testing()\n\ntest.set\n\n# A tibble: 100 × 4\n   flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n               &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;\n 1               186           39.5          17.4        3800\n 2               195           40.3          18          3250\n 3               195           38.7          19          3450\n 4               194           46            21.5        4200\n 5               189           35.9          19.2        3800\n 6               185           38.2          18.1        3950\n 7               188           39.5          17.8        3300\n 8               186           36            18.5        3100\n 9               191           42.3          21.2        4150\n10               186           39.6          17.7        3500\n# ℹ 90 more rows\n\n\n\n\n\nIn order to perform the predictions on the body weight of the penguin species, we will use two different regression model types; The Random Forest and the Linear Regression model. The regression algorithm is applied when the response variable is numeric. We will look at one model after another and then at the end we will compare the best model among the two.\n\n\nFirst, we will specify the model type as random forest using the rand_forest() function of the parsnip package (Kuhn and Vaughan, 2022). The set engine ranger and the mode regression will be used.\nFinally, the model will be fitted with the train data set using the fit() function of the parsnip package (Kuhn and Vaughan, 2022) with the response variable as body_mass_g.\n\nmod.rf = rand_forest() %&gt;% \n  set_engine(engine = \"ranger\") %&gt;% \n  set_mode(mode = \"regression\") %&gt;% \n  fit(body_mass_g~., data = train.set)\n\nmod.rf\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      233 \nNumber of independent variables:  3 \nMtry:                             1 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       119311.9 \nR squared (OOB):                  0.8177082 \n\n\nThe summary of the fitted model shows the R-squared value is 0.80. This shows that 80% of our data fits better to the model.\n\n\nThen, we will evaluate the model’s performance on the testing data using the predict() function of stats package (R Core Team, 2022). The model have achieved predictions of body weight of the penguins using the test data set which was not used in data training. The values of the predictions are within the distribution ranges of the response variable.\n\npred.rf = mod.rf %&gt;% \n  predict(test.set) %&gt;% \n  bind_cols(test.set)\n\npred.rf\n\n# A tibble: 100 × 5\n   .pred flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n   &lt;dbl&gt;             &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;\n 1 3583.               186           39.5          17.4        3800\n 2 3833.               195           40.3          18          3250\n 3 3931.               195           38.7          19          3450\n 4 3980.               194           46            21.5        4200\n 5 3709.               189           35.9          19.2        3800\n 6 3693.               185           38.2          18.1        3950\n 7 3587.               188           39.5          17.8        3300\n 8 3509.               186           36            18.5        3100\n 9 4040.               191           42.3          21.2        4150\n10 3595.               186           39.6          17.7        3500\n# ℹ 90 more rows\n\n\n\n\n\nAfter validating the model performance, then we need to test its accuracy using the metrics() function of yardstick package (Kuhn et al., 2022). The data used will be the output of the trained model (pred.rf), the truth variable will be the actual values of the body mass and the estimate variable will be the the predicted values of the body mass.\n\nbb = pred.rf %&gt;% \n  metrics(truth = body_mass_g,\n          estimate = .pred)\n\nbb\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     343.   \n2 rsq     standard       0.815\n3 mae     standard     279.   \n\n\nThe accuracy result shows the R-squared value of 0.81 meaning that our model is accurate for more than 80%. Other performance indicators like Root Mean Squared Error (RMSE) and the Mean Absolute Error (MAE) are also low. The RMSE measures the average difference between values predicted by a model and the actual values while the MAE measures the average absolute error between actual and predicted values.\n\n\n\nFinally, we will use the tuned model to make predictions on new data. First we will create the new dataset using the rnorm() function of R software (R Core Team, 2022). We will name our new data as data.new.\n\ndata.new = tibble(bill_depth_mm = rnorm(n = 10,\n                                  mean = 15, \n                                  sd = 3),\n            bill_length_mm = rnorm(n = 10,\n                                   mean = 50, \n                                   sd = 10),\n            flipper_length_mm = rnorm(n = 10,\n                                      mean = 195,\n                                      sd = 50))\n\nmod.rf %&gt;% \n  predict(data.new) %&gt;% \n  bind_cols(data.new) %&gt;% \n  mutate(.pred = as.integer(.pred))\n\n# A tibble: 10 × 4\n   .pred bill_depth_mm bill_length_mm flipper_length_mm\n   &lt;int&gt;         &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;\n 1  5417          14.0           57.9              263.\n 2  3305          14.1           29.0              166.\n 3  4022          15.9           49.6              155.\n 4  3818          20.5           46.0              168.\n 5  5266          12.1           48.9              235.\n 6  4179          21.6           68.0              160.\n 7  3743          14.4           41.9              131.\n 8  5599          17.9           69.0              314.\n 9  4192          12.4           57.1              140.\n10  4745          13.5           57.4              205.\n\n\nThat’s great!!\nOur model has predicts the body mass for the penguin species for the new data.\n\n\n\n\nWhen using the linear regression model we will first specify the model type as Linear Regression using the linear_reg() function of the parsnip package (Kuhn and Vaughan, 2022). The set engine lm and the mode regression will be used.\nFinally, the model will be fitted with the train data set using the fit() function of the parsnip package (Kuhn and Vaughan, 2022) with the response variable as body_mass_g.\n\nmod.lm = linear_reg() %&gt;% \n  set_engine(engine = \"lm\") %&gt;% \n  set_mode(mode = \"regression\") %&gt;% \n  fit(body_mass_g~., data = train.set %&gt;% select(-flipper_length_mm))\n\nmod.lm\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = body_mass_g ~ ., data = data)\n\nCoefficients:\n   (Intercept)  bill_length_mm   bill_depth_mm  \n       2704.67           82.83         -123.51  \n\n\n\n\nThen, we will evaluate the model’s performance on the testing data using the predict() function of stats package (R Core Team, 2022). Our model has predicted for the body weight values of the penguins.\n\npred.lm = mod.lm %&gt;% \n  predict(test.set) %&gt;% \n  bind_cols(test.set)\n\npred.lm\n\n# A tibble: 100 × 5\n   .pred flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n   &lt;dbl&gt;             &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;\n 1 3827.               186           39.5          17.4        3800\n 2 3819.               195           40.3          18          3250\n 3 3563.               195           38.7          19          3450\n 4 3859.               194           46            21.5        4200\n 5 3307.               189           35.9          19.2        3800\n 6 3633.               185           38.2          18.1        3950\n 7 3778.               188           39.5          17.8        3300\n 8 3401.               186           36            18.5        3100\n 9 3590.               191           42.3          21.2        4150\n10 3798.               186           39.6          17.7        3500\n# ℹ 90 more rows\n\n\n\n\n\nThe model accuracy will be tested using the metrics() function of yardstick package (Kuhn et al., 2022).\n\ncc = pred.lm %&gt;% \n  metrics(truth = body_mass_g,\n          estimate = .pred)\n\ncc\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     616.   \n2 rsq     standard       0.411\n3 mae     standard     506.   \n\n\nThe accuracy result shows the R-square is 0.41, RMSE is 616.14 and the MAE value is 505.88.\n\n\n\n\n\nWe have seen the performance of each model. We have to choose which model is more accurate and performs better than the other. I will join the two accuracy test results; the one from the random forest and the other from the linear regression model (Table 1).\n\npred.rf %&gt;% \n  metrics(truth = body_mass_g,\n          estimate = .pred) %&gt;% \n  pivot_wider(names_from = .metric, \n              values_from = .estimate) %&gt;% \n  mutate(model = \"Random Forest\") %&gt;% \n  select(Model = 5, Rsquare=3, RMSE=2, MAE=4) %&gt;% \n  bind_rows(\n    \npred.lm %&gt;% \n  metrics(truth = body_mass_g,\n          estimate = .pred)  %&gt;% \n  pivot_wider(names_from = .metric, \n              values_from = .estimate) %&gt;% \n  mutate(model = \"Linear Regression\") %&gt;% \n  select(Model = 5, Rsquare=3, RMSE=2, MAE=4)\n) %&gt;% \n  mutate(across(is.numeric, round, 2))  %&gt;% \n  gt::gt()\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(is.numeric, round, 2)`.\nCaused by warning:\n! Use of bare predicate functions was deprecated in tidyselect 1.1.0.\nℹ Please use wrap predicates in `where()` instead.\n  # Was:\n  data %&gt;% select(is.numeric)\n\n  # Now:\n  data %&gt;% select(where(is.numeric))\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\n\n\n\nTable 1:  The table showing the model performance indicators \n  \n    \n    \n      Model\n      Rsquare\n      RMSE\n      MAE\n    \n  \n  \n    Random Forest\n0.81\n343.03\n279.11\n    Linear Regression\n0.41\n616.14\n505.88\n  \n  \n  \n\n\n\n\n\nThe table shows that the Random Forest is the best than the Linear Regression Model!! This is because it has higher R-squared value with low RMSE and MAE values (Table 1).\n\n\n\nAll in all, the success of machine learning and artificial intelligence depend on the quality of the data, the complexity of the problem, and the choice of the appropriate algorithms and techniques (Jiawei Han and Pei, 2011; Witten et al., 2005).\nThe quality of the data is one of the most important factors for the success of machine learning. The data should be accurate, complete, and representative of the problem domain (Jiawei Han and Pei, 2011; Witten et al., 2005). In addition, the data should be properly labeled and preprocessed to ensure that the machine learning algorithms can effectively learn from it.\nThe complexity of the problem is also a key factor in determining the success of machine learning (Gomez-Cabrero et al., 2014). Some problems are inherently more complex than others, and require more sophisticated algorithms and techniques to solve. For example, image recognition and natural language processing are typically more complex than simple regression problems.\nFinally, the choice of the appropriate algorithms and techniques is critical for the success of machine learning (Gomez-Cabrero et al., 2014; Sarker, 2021). Different algorithms and techniques are suited for different types of problems, and the choice of the appropriate one will depend on the specific problem and the available data. Additionally, the parameters and hyperparameters of the algorithms need to be properly tuned to ensure that the models are optimized for the problem at hand.\n\n\n\n\n\n\nNote\n\n\n\nDon’t miss out our next post in this blog!!!"
  },
  {
    "objectID": "posts/summertime/index.html",
    "href": "posts/summertime/index.html",
    "title": "The Memorable Summer Time",
    "section": "",
    "text": "The summer time holiday begin soon after first term, which usually ends at late May…….."
  },
  {
    "objectID": "posts/summertime/index.html#the-maize-price",
    "href": "posts/summertime/index.html#the-maize-price",
    "title": "The Memorable Summer Time",
    "section": "The Maize Price",
    "text": "The Maize Price\nDoluptistia nis quo mil is magnihi tature debis num ipis porenda sectem quae doluptur? Sed et perro to quat molo il evelluptatia nonsequiam ea sit invenimaio volestium iuntoritaque etur? Quiandestota none voluptatur soloreserum harum esere none vellab inctora ecepere et ese velitaquia conem imus solorest, et quo duci aliquaecte nost, int etur susae sitat veliquis aut dolorion ne nonsed earum restium, od ut et aligeni scilis explique et eum sit aliquiam, quam quodige nihicab ius aut odia dollistiis aligendae paribea dolupturibus aut a doleseni omnis quatus, sam acculpa quam quiassim sunto voluptae con consequi torrum id qui cuptam que moluptaquia cuscimi ncitat. Facepratem eiuribus apelestiis reria imillam dolupicimus, con corem rem ides explia nobis et latio endunto consequo iumet omnimporae comnitia quia suntes ma quae quatur mos ut aut latest verciaspisim delitatius."
  },
  {
    "objectID": "posts/summertime/index.html#rice-price",
    "href": "posts/summertime/index.html#rice-price",
    "title": "The Memorable Summer Time",
    "section": "Rice price",
    "text": "Rice price\nLignimet qui que velenesed maxim faccum repudam, aligendit lam il ilis que veliquiam sanderundam rem alicaborpos id mo officiis rest es que volorepero core int unto magnati derionsecto cullaborem simus sit, sectae oditem arum exerspernam, temporernat. Otam que pratur res ea velis eicid militi quis ea autem rerro blacilit voluptat ipienda sum autaspe llique volecaeprem rem que soluptius abo. Ut qui rest aut quae nime et repudit quia volumquid ut omnimus."
  },
  {
    "objectID": "posts/summertime/index.html#the-worse-goes-to-school-fees",
    "href": "posts/summertime/index.html#the-worse-goes-to-school-fees",
    "title": "The Memorable Summer Time",
    "section": "The worse goes to school fees",
    "text": "The worse goes to school fees\nFugit maximint que nis et aligendis poressum quos ut id quos vellorum res enimpelia venientem quibusa\n\n\n\nThe family is chilling at th east coast of Nevada\n\n\nFigure 1 shows a scatter plot and raw data of penguins\n\n\n\n\n\nFigure 1: The penguisn of the southe americans and the habitats the reside\n\n\n\n\n\n\n?(caption)"
  },
  {
    "objectID": "posts/the-family-affairs/index.html",
    "href": "posts/the-family-affairs/index.html",
    "title": "The Family Affairs: A Look into the Importance of Family",
    "section": "",
    "text": "Family is the most important institution in our lives. It is where we learn our first lessons about love, trust, and respect. It is where we find comfort and support when we need it the most. Family is the foundation of our society, and it is important to nurture and cherish it. As we navigate through life, family affairs are bound to arise. Whether it’s a joyous occasion like a wedding or the arrival of a new baby, or a more difficult situation like a family member falling ill, these events can bring us closer together or create tension and stress.\n\nIn today’s fast-paced world, we often forget the importance of family. We get caught up in our work, our social lives, and our own personal goals. We forget that our family is always there for us, no matter what. We take them for granted and fail to appreciate the role they play in our lives. But family is more than just a group of people who share the same bloodline. It is a bond that goes beyond genetics. It is a bond that is built on love, trust, and mutual respect. It is a bond that lasts a lifetime.\n\nOne of the most important aspects of family is communication. Good communication is the key to any successful relationship, and it is especially important in a family setting. We need to talk to each other, listen to each other, and respect each other’s opinions. We need to be open and honest with each other, even when it’s difficult. Maintaining healthy relationships with family members can be challenging, especially if there are underlying issues such as unresolved conflicts or differing values. One way to improve relationships is to communicate openly and honestly with each other. This means actively listening to each other’s concerns and perspectives, and working together to find common ground. It’s also important to set boundaries and respect each other’s boundaries.\n\nAnother important aspect of family is spending time together. In today’s busy world, it can be difficult to find time to spend with our loved ones. But it’s important to make the effort. Whether it’s a family dinner, a game night, or a vacation, spending time together strengthens our bond and reminds us of the importance of family.\n\nFamily also provides us with a sense of belonging. It gives us a place to call home, a place where we are accepted for who we are. It gives us a support system when we need it the most. It provides us with a sense of security and stability.\n\n\n\nBut family is not always easy. There are times when we disagree, when we argue, and when we hurt each other. But it’s important to remember that family is worth fighting for. It’s important to work through our differences and to forgive each other when we make mistakes.\n\n\n\n\n\n\n\n\n\n\nIt’s important to remember the importance of forgiveness. We all make mistakes, and holding grudges can create long-term damage to relationships. Learning to forgive and move forward can help strengthen family bonds and create a more positive atmosphere.\n\n\nFamily is the most important institution in our lives. It provides us with love, support, and a sense of belonging. It is a bond that lasts a lifetime, and it is worth nurturing and cherishing. So take the time to communicate with your loved ones, spend time with them, and appreciate the role they play in your life. Remember that family is always there for you, no matter what. Family affairs can be complex and emotional, but they don’t have to be overwhelming. By seeking the advice of professionals, communicating openly with family members, and setting boundaries, you can navigate through these issues in a professional and effective manner.\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that maintaining healthy relationships with family members is key to a happy and fulfilling life. One thing that is important to remember is that every family is unique. What works for one family may not work for another. That being said, there are some general tips that can help make family affairs go more smoothly."
  },
  {
    "objectID": "posts/the-family-affairs/index.html#the-maize-price",
    "href": "posts/the-family-affairs/index.html#the-maize-price",
    "title": "The Family Affairs: A Look into the Importance of Family",
    "section": "The Maize Price",
    "text": "The Maize Price\nDoluptistia nis quo mil is magnihi tature debis num ipis porenda sectem quae doluptur? Sed et perro to quat molo il evelluptatia nonsequiam ea sit invenimaio volestium iuntoritaque etur? Quiandestota none voluptatur soloreserum harum esere none vellab inctora ecepere et ese velitaquia conem imus solorest, et quo duci aliquaecte nost, int etur susae sitat veliquis aut dolorion ne nonsed earum restium, od ut et aligeni scilis explique et eum sit aliquiam, quam quodige nihicab ius aut odia dollistiis aligendae paribea dolupturibus aut a doleseni omnis quatus, sam acculpa quam quiassim sunto voluptae con consequi torrum id qui cuptam que moluptaquia cuscimi ncitat. Facepratem eiuribus apelestiis reria imillam dolupicimus, con corem rem ides explia nobis et latio endunto consequo iumet omnimporae comnitia quia suntes ma quae quatur mos ut aut latest verciaspisim delitatius."
  },
  {
    "objectID": "posts/the-family-affairs/index.html#rice-price",
    "href": "posts/the-family-affairs/index.html#rice-price",
    "title": "The Family Affairs: A Look into the Importance of Family",
    "section": "Rice price",
    "text": "Rice price\nLignimet qui que velenesed maxim faccum repudam, aligendit lam il ilis que veliquiam sanderundam rem alicaborpos id mo officiis rest es que volorepero core int unto magnati derionsecto cullaborem simus sit, sectae oditem arum exerspernam, temporernat. Otam que pratur res ea velis eicid militi quis ea autem rerro blacilit voluptat ipienda sum autaspe llique volecaeprem rem que soluptius abo. Ut qui rest aut quae nime et repudit quia volumquid ut omnimus."
  },
  {
    "objectID": "posts/the-family-affairs/index.html#the-worse-goes-to-school-fees",
    "href": "posts/the-family-affairs/index.html#the-worse-goes-to-school-fees",
    "title": "The Family Affairs: A Look into the Importance of Family",
    "section": "The worse goes to school fees",
    "text": "The worse goes to school fees\nFugit maximint que nis et aligendis poressum quos ut id quos vellorum res enimpelia venientem quibusa\n\n\n\nThe family is chilling at th east coast of Nevada\n\n\nEquation 1 is the mathematical equation respresenting….\n\\[\n\\sigma = \\frac{\\phi}{\\alpha}\n\\tag{1}\\]"
  },
  {
    "objectID": "posts/worldcloud/index.html",
    "href": "posts/worldcloud/index.html",
    "title": "Text analytics and word cloud in R and ggplot2",
    "section": "",
    "text": "wordcloud is an image composed of words used in a particular text or subject, in which the size of each word indicates its frequency or importance. In this post, we are going to learn how to create worldcloud in R language (R Core Team, 2022). To generate wordcloud plot in R, we need either worcloud (Fellows, 2018) or wordcloud2 (Lang and Chien, 2018) packages. If these packages are not installed in your machine, you can simply download and install them from CRAN as;\n\ninstall.packages(\"wordcloud\")\ninstall.packages(\"wordcloud2\")\n\nBefore we proceed, we need to load some packages, whose function we are going to use throughout this post. These packages are highlighted in the chunk below;\n\nrequire(tidyverse)\nrequire(readxl)\nrequire(tm)           # for text mining\nrequire(SnowballC)    # for text stemming\nrequire(wordcloud)    # word-cloud generator\nrequire(RColorBrewer) # color palettes\nrequire(wordcloud2)\nrequire(ggalluvial)"
  },
  {
    "objectID": "posts/worldcloud/index.html#tidy-pproject-titles",
    "href": "posts/worldcloud/index.html#tidy-pproject-titles",
    "title": "Text analytics and word cloud in R and ggplot2",
    "section": "tidy pproject titles",
    "text": "tidy pproject titles\n\nstudent.text = student$Title \n\nThereafter, a corpus will be generated from the vector using the Corpus() function of tm package\n\nstudent.doc = tm::Corpus(VectorSource(student.text))\n\nCleaning is an essential step to take before you generate your wordcloud. Indeed, for your analysis to bring useful insights, you may want to remove special characters, numbers or punctuation from your text. In addition, you should remove common stop words in order to produce meaningful results and avoid the most common frequent words such as “I” or “the” to appear in the word cloud. If you’re working with a corpus, there are several packages you can use to clean your text. The following lines of code show you how to do this using the tm package.\n\nstudent.doc.clean = student.doc %&gt;% \n  tm_map(removeNumbers) %&gt;% \n  tm_map(removePunctuation) %&gt;% \n  tm_map(stripWhitespace) %&gt;% \n  tm_map(content_transformer(tolower)) %&gt;% \n  tm_map(removeWords, stopwords(kind = \"en\"))\n\n\nCreate a document-term-matrix\nWhat you want to do as a next step is to have a dataframe containing each word in your first column and their frequency in the second column. This can be done by creating a document term matrix with the TermDocumentMatrix function from the tm package.\n\nstudent.matrix = TermDocumentMatrix(student.doc.clean) %&gt;% \n  as.matrix()\n\nstudent.word = sort(rowSums(student.matrix), decreasing = TRUE)\n     \n student.word.df = data.frame(word = names(student.word), freq = student.word)\n\n\n\nGenerate the word cloud\nThe wordcloud package is the most classic way to generate a word cloud. The following line of code shows you how to properly set the arguments. As an example, I chose to work with the student titles on their third year research project at SoAF.\n\nset.seed(1234) # for reproducibility\n\nwordcloud(words = student.word.df$word, \n          freq = student.word.df$freq, \n          min.freq = 1,\n          random.order = FALSE, \n         max.words = 200,\n         # rot.per = 0.35,\n          colors = brewer.pal(8, \"Dark2\"),\n         scale = c(3.5,0.4))\n\n\n\n\nIt may happen that your word cloud crops certain words or simply doesn’t show them. If this happens, make sure to add the argument scale=c(3.5,0.25) and play around with the numbers to make the word cloud fit. Another common mistake with word clouds is to show too many words that have little frequency. If this is the case, make sure to adjust the minimum frequency argument (min.freq=…) in order to render your word cloud more meaningful.\n\nset.seed(1234) # for reproducibility\n\nwordcloud(words = student.word.df$word, \n          freq = student.word.df$freq, \n          min.freq = 3,\n          random.order = FALSE, \n         # max.words = 1000,\n         # rot.per = 0.35,\n          colors = brewer.pal(8, \"Dark2\"), \n         scale = c(3.5,0.6))\n\n\n\n\nThe wordcloud2 package is a bit more fun to use, allowing us to do some more advanced visualisations. For instance, you can choose your wordcloud to appear in a specific shape or even letter (see this vignette for a useful tutorial). As an example, I used the same corpus of student title and generated the two word clouds shown below. Cool, right?\n\nwordcloud2(data = student.word.df, \n           size = 0.8, \n           color = \"random-dark\",\n           minSize = 3,\n           ellipticity = 2,\n           gridSize = 11)\n\n\n\n\n\nIn summary, R language provide myriad package for visualizing not only numeric data but also for textual information and summarize them in plots that are easily to understand. In the next post, we are going to use the approach to visualize textual information using alluvial plots"
  },
  {
    "objectID": "posts/global-temperature-record/index.html",
    "href": "posts/global-temperature-record/index.html",
    "title": "The Surpass Global Ocean Temperature Record: Implications for Ocean Productivity in Tanzania",
    "section": "",
    "text": "Global warming is a serious issue that has been affecting the planet for several decades. It is a phenomenon that has been caused by a variety of factors, both natural and man-made. One of the primary causes of global warming is the increase in greenhouse gases in the atmosphere (Pinkerton et al., 2021; Sakalli et al., 2017). These gases, such as carbon dioxide, methane, and nitrous oxide, trap heat from the sun and prevent it from escaping into space (Sakalli et al., 2017). This leads to a gradual increase in the Earth’s temperature, which is known as global warming. The main source of these gases is human activities, such as burning fossil fuels, deforestation, and industrial processes.\n\n\n\nBurning of the fossil fuels\n\n\nOver the past few days, multiple media outlets have been reporting on the global warming conditions experienced from March to June 2023. These reports indicate that June 2023 is projected to be even warmer than May 2023. According to the NOAA website, May 2023 has been recorded as the hottest May month since temperature records began in 1850. In this blog post, we will delve into the historical temperature trends from 1850 to 2023 for the month of May, followed by an exploration of the impact of rising temperatures on ocean productivity in the vicinity of Tanzania.\nBefore we proceed, we will load some packages, whose functions are going to be used throughout this post. These packages are highlighted in the chunk below;\n\nrequire(tidyverse)\nrequire(ggstatsplot)"
  },
  {
    "objectID": "posts/global-temperature-record/index.html#data",
    "href": "posts/global-temperature-record/index.html#data",
    "title": "The Surpass Global Ocean Temperature Record: Implications for Ocean Productivity in Tanzania",
    "section": "Data",
    "text": "Data\nFor this analysis, we obtained monthly global sea surface anomaly data from January 1850 to May 2023 from the NOAA website. Additionally, we collected MODIS daily sea surface data and primary productivity data from the southern part of Tanzania, specifically the Mtwara region, within the longitude range of 40.5°E to 40.7°E and latitude range of 10.4°S to 9.8°S. The data span from January 2015 to June 10, 2023. We loaded both the global and Mtwara datasets into our session using the read_csv() function from the readr package (Wickham et al., 2023). Our analysis focused solely on the month of May for the global data and June for Mtwara dataset.\n\nglobal.data = read_csv(\"global_temp.csv\") \n\n\nmtwara.sst.day = read_csv(\"mtwara_sst_jun.csv\") \n\n\nmtwara.pp.day = read_csv(\"mtwara_pp.csv\")"
  },
  {
    "objectID": "posts/global-temperature-record/index.html#consultated-references",
    "href": "posts/global-temperature-record/index.html#consultated-references",
    "title": "New global higher temperature records; its effects on ocean productivity and fishery yield: The situation in Tanzania",
    "section": "Consultated references",
    "text": "Consultated references\n\n\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr."
  },
  {
    "objectID": "posts/global-temperature-record/index.html#global-sea-surface-temperature-sst-trend",
    "href": "posts/global-temperature-record/index.html#global-sea-surface-temperature-sst-trend",
    "title": "The Surpass Global Ocean Temperature Record: Implications for Ocean Productivity in Tanzania",
    "section": "Global Sea Surface Temperature (SST) Trend",
    "text": "Global Sea Surface Temperature (SST) Trend\nThe analysis reveals a decreasing trend in sea surface temperature (SST) anomaly from 1850 to 1910, with values consistently below the global average temperature indicated by a red dashed line in Figure 1. Starting from the year 1910, the global temperature anomaly for May began to rise, although still below the average global temperature. However, from the year 1975 onwards, the average global temperature anomaly for May surpassed the global mean temperature, exceeding the mean value (Figure 1). Despite some fluctuations over the years, the May temperature continued to rise, reaching its highest recorded values in May 2023 compared to previous years as shown in Figure Figure 1. This significant increase in May 2023 temperature has sparked widespread discussions worldwide regarding its impact on global biodiversity and ecosystems.\n\n\n\n\n\nFigure 1: The trend of global ocean temperature for the month of May from 1850 to 2023"
  },
  {
    "objectID": "posts/global-temperature-record/index.html#sst-in-tanzania-and-its-impact-in-primary-production",
    "href": "posts/global-temperature-record/index.html#sst-in-tanzania-and-its-impact-in-primary-production",
    "title": "The Surpass Global Ocean Temperature Record: Implications for Ocean Productivity in Tanzania",
    "section": "SST in Tanzania and its impact in Primary Production",
    "text": "SST in Tanzania and its impact in Primary Production\nDue to this alarming rise in temperature values recorded for May 2023 around the world and its potential impact on global biodiversity and ecosystems, it is imperative to delve deeper into the issue and understand its implications. In this blog post, we will specifically focus on the impact of this rising temperature on primary production in the ocean which could then affects fishery population.\nPrimary production refers to the process by which plants and other photosynthetic organisms convert sunlight into energy, which is then used to fuel the growth of other organisms in the ecosystem (Kim and Kim, 2021). In aquatic ecosystems, primary production is largely driven by phytoplankton, which are tiny, single-celled organisms that float near the surface of the water.\nOur analysis will focus on the SST and primary productivity data from the month of June from 2015 to June 20, 2023."
  },
  {
    "objectID": "posts/global-temperature-record/index.html#variation-in-sst-at-mtwara",
    "href": "posts/global-temperature-record/index.html#variation-in-sst-at-mtwara",
    "title": "The Surpass Global Ocean Temperature Record: Implications for Ocean Productivity in Tanzania",
    "section": "Variation in SST at Mtwara",
    "text": "Variation in SST at Mtwara\nThe temperature values in Mtwara between 2015 and 2022 were analyzed, and the results showed that the majority of the values fell within the range of 27 to 28 degrees Celsius, as illustrated in Figure 2. However, in 2023, the sea surface temperature (SST) recorded higher values exceeding 28 degrees Celsius. A comparison of the temperatures between 2022 and 2023 revealed that 2022 had a lower values below 28 degrees Celsius, while that of 2023 had values greater than 28.9 degrees Celsius.\n\n\n\n\n\nFigure 2: The histogram showing variation of temperature at Mtwara for the month of June\n\n\n\n\nWe decided to divide the years into two periods: 2015-2022 and 2023. Through further analysis, we found that the average temperature during the eight-year period from 2015 to 2022 was significantly lower compared to that of the year 2023, as shown in Figure 3. Interestingly, the maximum value for the eight-year period (2015-2022) (27.15oC) was lower than the median value of the year 2023 (27.89oC), as illustrated in Figure 3. These elevated sea surface temperature (SST) values observed between May and June 2023 have sparked widespread concerns globally regarding their impacts on ecosystems.\n\n\n\n\n\nFigure 3: The boxplot showing variation in temperature at Mtwara for the month of June between the average of eight years before 2023 with that of 2023."
  },
  {
    "objectID": "posts/global-temperature-record/index.html#variation-in-primary-production-at-mtwara",
    "href": "posts/global-temperature-record/index.html#variation-in-primary-production-at-mtwara",
    "title": "The Surpass Global Ocean Temperature Record: Implications for Ocean Productivity in Tanzania",
    "section": "Variation in Primary Production at Mtwara",
    "text": "Variation in Primary Production at Mtwara\nThen, let’s look on the variation of levels of primary production at Mtwara with this rise in the ocean temperature. The results revealed that, between the year 2015 and 2022, the amount of primary production at Mtwara ranged around 220 and 520 g C/m2/year (Figure 4). Surprisingly, in the year 2023, the primary production ranged between 100 and 320 g C/m2/year with many values recorded below 270 g C/m2/year (Figure 4).\n\n\n\n\n\nFigure 4: The histogram showing variation in the amount of primary production at Mtwara for the month of June\n\n\n\n\nWhen further analysis was done by grouping the data into two groups (average 2015-2022 and 2023), the amount of primary production showed to be lower in the year 2023 compared to the average of eight years before Figure 5. While the mean value in primary production for the month of June before the year 2023 was 342.57, that for the June 2023 was recorded as 286.24 g C/m2/year Figure 5.\n\n\n\n\n\nFigure 5: The boxplot showing variation in the amount of primary production at Mtwara for the month of June between the average of eight years before 2023 with that of 2023.\n\n\n\n\nThe result reveals that, higher temperature recorded in June 2023 matches well with the decline in the amount of primary production in the same month and this in-turn might affects the fish population and other aquatic organisms. Fish, as well as other aquatic animals, rely on phytoplankton organisms as a primary food source (Kim and Kim, 2021). As such, the amount of primary production in an ecosystem can have a significant impact on the overall fish catch (Calderwood et al., 2019; Marshak and Link, 2021). In areas where primary production is high, fish populations tend to be larger and more abundant (Blanchard et al., 2012). Conversely, in areas where primary production is low, fish populations may be smaller and less productive.\n\n\n\n\n\n\nNote\n\n\n\nThere are many other factors that can influence fish catch, including fishing pressure, habitat degradation, and climate change. Therefore, a holistic approach is needed to ensure sustainable fisheries management to our oceans."
  },
  {
    "objectID": "posts/global-temperature-record/index.html#introduction",
    "href": "posts/global-temperature-record/index.html#introduction",
    "title": "The Surpass Global Ocean Temperature Record: Implications for Ocean Productivity in Tanzania",
    "section": "",
    "text": "Global warming is a serious issue that has been affecting the planet for several decades. It is a phenomenon that has been caused by a variety of factors, both natural and man-made. One of the primary causes of global warming is the increase in greenhouse gases in the atmosphere (Pinkerton et al., 2021; Sakalli et al., 2017). These gases, such as carbon dioxide, methane, and nitrous oxide, trap heat from the sun and prevent it from escaping into space (Sakalli et al., 2017). This leads to a gradual increase in the Earth’s temperature, which is known as global warming. The main source of these gases is human activities, such as burning fossil fuels, deforestation, and industrial processes.\n\n\n\nBurning of the fossil fuels\n\n\nOver the past few days, multiple media outlets have been reporting on the global warming conditions experienced from March to June 2023. These reports indicate that June 2023 is projected to be even warmer than May 2023. According to the NOAA website, May 2023 has been recorded as the hottest May month since temperature records began in 1850. In this blog post, we will delve into the historical temperature trends from 1850 to 2023 for the month of May, followed by an exploration of the impact of rising temperatures on ocean productivity in the vicinity of Tanzania.\nBefore we proceed, we will load some packages, whose functions are going to be used throughout this post. These packages are highlighted in the chunk below;\n\nrequire(tidyverse)\nrequire(ggstatsplot)"
  },
  {
    "objectID": "posts/global-temperature-record/index.html#variation-in-primary-production-at-mtwara-due-to-elevated-temperature",
    "href": "posts/global-temperature-record/index.html#variation-in-primary-production-at-mtwara-due-to-elevated-temperature",
    "title": "The Surpass Global Ocean Temperature Record: Implications for Ocean Productivity in Tanzania",
    "section": "Variation in Primary Production at Mtwara due to elevated Temperature",
    "text": "Variation in Primary Production at Mtwara due to elevated Temperature\nNow, let’s look on the variation of levels of primary production at Mtwara with this rising in the ocean temperature. The results revealed that, between the year 2015 and 2022, the amount of primary production at Mtwara for the month of June ranged between 225 and 500 g C/m2/year with many values around 325 g C/m2/year (Figure 4). Surprisingly, in the year 2023, the amount of primary production ranged between less than 100 and 400 g C/m2/year with many values recorded below 225 g C/m2/year (Figure 4). Further, the result revealed that the minimum value for the eight years period, was the same as the median of the year 2023 (Figure 4).\n\n\n\n\n\nFigure 4: The histogram showing variation in the amount of primary production at Mtwara for the month of June\n\n\n\n\nWhen further analysis was done by grouping the data into two groups (average 2015-2022 and 2023), the amount of primary production showed to be significant lower in the year 2023 compared to the average of eight years before Figure 5. While the median value in primary production for the month of June before the year 2023 was 330.75, that for the June 2023 was recorded as 239.29 g C/m2/year Figure 5.\n\n\n\n\n\nFigure 5: The boxplot showing variation in the amount of primary production at Mtwara for the month of June between the average of eight years before 2023 with that of 2023.\n\n\n\n\nThe study results indicate that the higher temperature recorded in June 2023 is closely associated with a decline in the amount of primary production during the same month. This decline in primary production can have implications for the fish population and other aquatic organisms. Fish and other aquatic animals rely on phytoplankton, which are primary producers, as a primary food source. Therefore, the quantity of primary production in an ecosystem plays a crucial role in determining the overall fish catch.\nPrevious studies (Calderwood et al., 2019; Kim and Kim, 2021; Marshak and Link, 2021) have demonstrated the correlation between primary production and fish populations. Areas with high levels of primary production tend to support larger and more abundant fish populations. Conversely, in regions where primary production is low, fish populations may be smaller and less productive (Blanchard et al., 2012).\n\n\n\n\n\n\nNote\n\n\n\nThere are many other factors that can influence fish catch, including fishing pressure, habitat degradation, and climate change. Therefore, a holistic approach is needed to ensure sustainable fisheries management to our oceans."
  },
  {
    "objectID": "posts/github-deploy-publish/index.html#benefits-of-using-github",
    "href": "posts/github-deploy-publish/index.html#benefits-of-using-github",
    "title": "Deploying and publishing to GitHub",
    "section": "",
    "text": "Deploying to GitHub has several benefits, including:\n\nVersion control: GitHub provides an easy-to-use platform for version control, which allows you to track changes to your code over time and collaborate with other developers.\nFree hosting: GitHub offers free hosting for static websites through its GitHub Pages service. This means you can host your website on GitHub without having to pay for hosting services.\nAccessibility: GitHub is widely used by developers around the world, which means your code and website are accessible to a large audience. It also makes it easy for others to contribute to your project.\nShowcase your work: GitHub provides a platform to showcase your work to potential employers or clients. You can also use GitHub to build a portfolio of your work, which can be a valuable asset in your career.\nContinuous integration: GitHub supports continuous integration, which allows you to automatically build and deploy your code whenever changes are made. This can save time and effort in the deployment process.\n\nGenerally, deploying to GitHub can help streamline the development process, provide a platform to showcase your work, and offer free hosting for your projects."
  },
  {
    "objectID": "posts/github-deploy-publish/index.html#github-deployment-steps",
    "href": "posts/github-deploy-publish/index.html#github-deployment-steps",
    "title": "Deploying and publishing to GitHub",
    "section": "",
    "text": "Steps in GitHub deployment\n\n\nDeploying to GitHub typically involves the following steps:\nFirst, create a GitHub account: If you don’t already have one. You may create an account by opening GitHub page. Click the Sign up button at the top right corner of the page. This will navigate to the sign up window at GitHubSignUp. Fill in your particulars including user name and password.\n\n\n\nSign up to GitHub\n\n\nAfter you have already created an account, create a new repository in GitHub from which you will deploy your work: Navigate to the GitHub homepage and click the New Repository button in the top-right corner of the page. This will open a new window. Give your repository a name; I have named my repository as github-deployment. Choose whether it should be public or private, and add a description if you like and click create repository\n\n\n\nCreating a new repository in GitHub\n\n\nInitialize Git: In your local development environment (Local Host) by opening the GitBash. Navigate to the directory where your project is located in your local host using cd function of the GitBash. Run the command git init to initialize Git. Check the status of your work by running git status in GitBash. It is adviced to check the status of your work after every step before proceeding to the new step to crosscheck if there is no any error or mistake made in the previous step\nAdd files to the repository: Use the git add command to add the files you want to include in your repository. You can use the command git add . (add .(period)) to add all the files in your current directory. Check for status of your work\nCommit changes: Use the git commit command to commit your changes to the repository. Remember to include a descriptive commit message to explain what changes you made. Check for the status of your work\nAdd the remote repository: Use the command git remote add origin https://github.com/your-username/your-repository.git to add the remote repository as the origin. Check for status of your work\nIf the working tree is clean, Push changes to the remote repository: Use the command git push -u origin main to push your changes to the remote repository. The -u flag sets the upstream branch to main, which means you’ll only need to run git push in the future to push changes.\nVerify deployment: Once you’ve pushed your changes, navigate to your repository on GitHub and verify that the changes have been deployed successfully.\nThat’s it! Your project is now deployed to GitHub and your client may access it from the cloud.\nIf you made changes to your project after first commit, then you will only run three steps to make those changes to your GitHub repository; add, commit and push\nAdd all your files in the working directory using git add .\nCommit changes to your repository using git commit -m “Add commit message”\npush your work using git push\nNavigate to your repository on GitHub and verify that the changes have been deployed successfully.\nIf the changes made to your GitHub repository are successful, you may wish to publish your repository and make it accessible to the public as a web page"
  },
  {
    "objectID": "posts/machine-learning-classification/index.html#machine-learning-algorithms",
    "href": "posts/machine-learning-classification/index.html#machine-learning-algorithms",
    "title": "Machine Learning (ML) using classification Algorithm in R",
    "section": "",
    "text": "There are several types of machine learning algorithms, including supervised learning, unsupervised learning, and reinforcement learning. Supervised learning involves using labeled (predictor) training data to train a model to make predictions on new, unseen data. Unsupervised learning involves finding patterns and relationships in data without the use of labeled training data. Reinforcement learning involves training a model to make decisions based on feedback in the form of rewards or punishments.\nClassification and regression are two of the most common types of supervised learning algorithms in machine learning and artificial intelligence. Classification is a type of supervised learning algorithm used for predicting discrete or categorical outcomes. It involves mapping input variables to discrete output categories or labels. The objective of classification is to build a model that accurately assigns new data points to the correct class or label.\nOn the other hand, regression is a type of supervised learning algorithm used for predicting continuous or numeric outcomes. It involves mapping input variables to continuous output values. The objective of regression is to build a model that accurately predicts the value of the dependent variable based on the values of the independent variables.\n\n\n\nDifference between classification and regression\n\n\nIn this tutorial we are going to deal with classification algorithm in predicting the type of penguin species flipper length, bill dimensions and sex. We will use penguins data from the palmerpenguins package (Horst et al., 2020). It includes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\nFirst, we will load the packages which we are going to use in this tutorial; I will use require() function, but you may also use library() function depending on your preferences.\n\nrequire(tidyverse)\nrequire(tidymodels)\nrequire(palmerpenguins)\nrequire(ranger)\nrequire(patchwork)"
  },
  {
    "objectID": "posts/machine-learning-classification/index.html#data",
    "href": "posts/machine-learning-classification/index.html#data",
    "title": "Machine Learning (ML) using classification Algorithm in R",
    "section": "",
    "text": "After loading the packages, we will load the penguins data (Horst et al., 2020) and remove all the missing values in the dataset and equate them as penguin.data. The dataset consist of 333 rows and 8 columns\n\npenguin.data = palmerpenguins::penguins %&gt;% \n  drop_na()\n\npenguin.data\n\n# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 323 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThen, we will look on the internal structure of the dataset using glimpse() function of dplyr package. Our dataset consist of 8 variables (columns); 3 are factors data (species, island and sex), 2 numeric or double data (bill_length_mm, bill_depth_mm) and 3 integers (flipper_length_mm, body_mass_g, year). The variable species have 3 different levels which are Adelie, Gentoo and Chinstrap\n\nglimpse(penguin.data)\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nIn this tutorial we will use only three variables from penguins data; which are species, bill_length_mm, bill_depth_mm. We will run the select() function of dplyr package (Wickham et al., 2022) to select our variables of interest\n\npenguin.data = penguin.data %&gt;% \n  select(species, \n         bill_length_mm, \n         bill_depth_mm)\n\nBefore we apply ML algorithms, first we will crosscheck whether the predictor variable (in this case, species), have distinct features (like size) which will help in providing the more accurate output during predictions. When there is interconnection or inter-relation between response variables, some confusion might arise during predictions.\nWe will use scatter plot between bill_length_mm and bill_depth_mm to see the distribution of each species in the dataset. There is a distinct differences between the size of the three penguins species (Figure 1). Each species has its size range; therefore, the dataset fit best in our analysis.\n\npenguin.data %&gt;% \n  ggplot(aes(y = bill_length_mm, \n             x = bill_depth_mm, \n             color = species)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\nFigure 1: Scatter plot showing the distribution of Adelie, Gentoo and Chinstrap species"
  },
  {
    "objectID": "posts/machine-learning-classification/index.html#classification-algorithm",
    "href": "posts/machine-learning-classification/index.html#classification-algorithm",
    "title": "Machine Learning (ML) using classification Algorithm in R",
    "section": "",
    "text": "As explained earlier, supervised learning involves training a dataset before you have your predictions. Since our output is categorical (prediction of the species type), then we will use classification algorithm in our analysis.\n\n\nClassification algorithm as one of the supervised learning, needs two data types; the training and testing data set. Our penguin.data will be split into these two groups. The training dataset will have a proportion of 70% and the remaining 30% will be the test dataset. In total, our dataset has 333 observations in which 233 samples will be used to train our model while in testing the accuracy of the model 100 samples will used.\n\nsplit.penguin = penguin.data %&gt;% \n  initial_split(prop = 0.7)\n\nsplit.penguin\n\n&lt;Training/Testing/Total&gt;\n&lt;233/100/333&gt;\n\n\nThe training data below with 233 samples will be used for training the model;\n\ntrain.set = split.penguin %&gt;% \n  training()\n\ntrain.set\n\n# A tibble: 233 × 3\n   species   bill_length_mm bill_depth_mm\n   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie              36.2          17.2\n 2 Gentoo              48.7          15.1\n 3 Chinstrap           49.8          17.3\n 4 Chinstrap           54.2          20.8\n 5 Gentoo              44.9          13.3\n 6 Chinstrap           50.7          19.7\n 7 Adelie              38.6          17  \n 8 Gentoo              46.2          14.9\n 9 Gentoo              50.1          15  \n10 Adelie              39.8          19.1\n# ℹ 223 more rows\n\n\nThe testing data with 100 observations will be used to test the accuracy of the model\n\ntest.set = split.penguin %&gt;% \n  testing()\n\ntest.set\n\n# A tibble: 100 × 3\n   species bill_length_mm bill_depth_mm\n   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie            40.3          18  \n 2 Adelie            38.9          17.8\n 3 Adelie            38.6          21.2\n 4 Adelie            42.5          20.7\n 5 Adelie            37.8          18.3\n 6 Adelie            37.7          18.7\n 7 Adelie            35.9          19.2\n 8 Adelie            38.8          17.2\n 9 Adelie            35.3          18.9\n10 Adelie            40.5          17.9\n# ℹ 90 more rows\n\n\n\n\nUse the train.set data to train the model. Run the rand_forest() of the parsnip package (Kuhn and Vaughan, 2022), set engine as ranger and the mode as classification. Fit the train data set with the response be the species. The summary of the training model will be shown with the predictions error of 0.02 equivalent to 2%.\n\nmod = rand_forest() %&gt;% \n  set_engine(engine = \"ranger\") %&gt;% \n  set_mode(mode = \"classification\") %&gt;% \n  fit(species~., data = train.set)\n\nmod\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      233 \nNumber of independent variables:  2 \nMtry:                             1 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.03958214 \n\n\n\n\n\nThen, the test.set data will be used in predictions of the type of the species provided we have the bill length and bill depth. The predicted class and the test dataset will be binded together to see if there is a match or mismatch of the predicted versus the actual species type.\n\nrf.pred = mod %&gt;% \n  predict(test.set) %&gt;% \n  bind_cols(test.set)\n\nrf.pred\n\n# A tibble: 100 × 4\n   .pred_class species bill_length_mm bill_depth_mm\n   &lt;fct&gt;       &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie      Adelie            40.3          18  \n 2 Adelie      Adelie            38.9          17.8\n 3 Adelie      Adelie            38.6          21.2\n 4 Adelie      Adelie            42.5          20.7\n 5 Adelie      Adelie            37.8          18.3\n 6 Adelie      Adelie            37.7          18.7\n 7 Adelie      Adelie            35.9          19.2\n 8 Adelie      Adelie            38.8          17.2\n 9 Adelie      Adelie            35.3          18.9\n10 Adelie      Adelie            40.5          17.9\n# ℹ 90 more rows\n\n\n\n\n\nWe have already predicted for the results, then we need to test for the accuracy of the model. The model is considered accurate when its accuracy value is higher or equal to 80%. The obtained accuracy of our model is 0.98 (98%) and the kap 96%. Our model can then be used to predict the type of species provided you have the bill length and the bill depth of the penguins.\n\nrf.pred %&gt;% \n  metrics(truth = species,\n          estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.94 \n2 kap      multiclass     0.905\n\n\nNow, we need to see the accuracy for each predicted species versus the actual type. We will use conf_mat() of the yardstick package (Kuhn et al., 2022). The result indicate that 43 Adelie were correctly predicted while 1 Chinstrap was incorrectly predicted as Adelie. While 16 Chinstrap were correctly predicted, 1 of this species was wrongly predicted as Gentoo. On the other hand, all 39 Gentoo were correctly predicted.\n\nrf.pred %&gt;% \n  conf_mat(truth = species, \n           estimate = .pred_class)\n\n           Truth\nPrediction  Adelie Chinstrap Gentoo\n  Adelie        46         1      0\n  Chinstrap      2        17      2\n  Gentoo         0         1     31\n\n\nThen, we need to calculate the probability of each observation be accurate for every species. We will use predict() function of *stats package of R (R Core Team, 2022).\n\nrf.preda = mod %&gt;% \n  predict(test.set, \n          type = \"prob\") %&gt;% \n  bind_cols(test.set)\n\nrf.preda\n\n# A tibble: 100 × 6\n   .pred_Adelie .pred_Chinstrap .pred_Gentoo species bill_length_mm\n          &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;            &lt;dbl&gt;\n 1        1             0           0        Adelie            40.3\n 2        0.997         0.00313     0        Adelie            38.9\n 3        0.996         0.00396     0        Adelie            38.6\n 4        0.796         0.182       0.0223   Adelie            42.5\n 5        0.999         0.00075     0        Adelie            37.8\n 6        0.989         0.0107      0        Adelie            37.7\n 7        0.998         0.00156     0        Adelie            35.9\n 8        0.996         0.00327     0.000286 Adelie            38.8\n 9        0.997         0.003       0        Adelie            35.3\n10        1             0           0        Adelie            40.5\n# ℹ 90 more rows\n# ℹ 1 more variable: bill_depth_mm &lt;dbl&gt;\n\n\n\n\n\nThe probability results will then be used to plot the roc curve (Figure 2). The roc_curve() function of yardstick package (Kuhn et al., 2022) will be used supplied with species, .pred_Adelie, .pred_Chinstrap, and .pred_Gentoo. The autoplot() function of the workflowsets package (Kuhn and Couch, 2022) will then be applied to create the curve. The curve shows that Adelie species fitted better to the model than other the Gentoo and Chinstrap species (Figure 2).\n\nrf.preda %&gt;% \n  roc_curve(species, \n            .pred_Adelie, \n            .pred_Chinstrap, \n            .pred_Gentoo) %&gt;% \n  autoplot()+\n  ggpubr::theme_pubclean()+\n  theme(strip.background = element_blank())\n\n\n\n\nFigure 2: The ROC curve of Adelie, Chinstrap and Gentoo species\n\n\n\n\n\n\n\nIn order to validate whether the model works to other newly collected data, we will validate it using the created data consists of bill length and depth. The data will be applied to the model and test if the prediction of the type of species will be done. The new created data will be named as new.penguin containing three observations;\n\nnew.penguin = tibble(bill_length_mm = c(35,47.5, 70),\n                     bill_depth_mm = c(13,17, 18))\n\nnew.penguin\n\n# A tibble: 3 × 2\n  bill_length_mm bill_depth_mm\n           &lt;dbl&gt;         &lt;dbl&gt;\n1           35              13\n2           47.5            17\n3           70              18\n\n\nThen, the model will be applied and see if it will predict the type of species. We will use predict() function.\nHoolah!! the model give us the predicted species as Adelie and Chinstrap\n\nmod %&gt;% \n  predict(new.penguin)\n\n# A tibble: 3 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 Adelie     \n2 Chinstrap  \n3 Chinstrap  \n\n\nWe will also create another data named as aa with 10 observations using rnorm() function of stats package (R Core Team, 2022). We will also predict the type of species for these data using our model. Again the model works and give us predictions!!\n\naa = tibble(bill_depth_mm = rnorm(n = 10,\n                                  mean = 15, \n                                  sd = 3),\n            bill_length_mm = rnorm(n = 10,\n                                   mean = 50, \n                                   sd = 10))\n\nmod %&gt;% \n  predict(aa) %&gt;% \n  bind_cols(aa)\n\n# A tibble: 10 × 3\n   .pred_class bill_depth_mm bill_length_mm\n   &lt;fct&gt;               &lt;dbl&gt;          &lt;dbl&gt;\n 1 Adelie               14.3           30.4\n 2 Gentoo               13.9           62.6\n 3 Gentoo               12.9           71.0\n 4 Chinstrap            18.0           50.5\n 5 Gentoo               13.4           53.5\n 6 Chinstrap            18.7           49.9\n 7 Gentoo               14.9           46.5\n 8 Adelie               17.2           30.2\n 9 Gentoo               14.7           53.5\n10 Gentoo               10.8           48.7"
  },
  {
    "objectID": "posts/machine-learning-classification/index.html#summary",
    "href": "posts/machine-learning-classification/index.html#summary",
    "title": "Machine Learning (ML) using classification Algorithm in R",
    "section": "",
    "text": "All in all, the success of machine learning and artificial intelligence depend on the quality of the data, the complexity of the problem, and the choice of the appropriate algorithms and techniques (Jiawei Han and Pei, 2011; Witten et al., 2005).\nThe quality of the data is one of the most important factors for the success of machine learning. The data should be accurate, complete, and representative of the problem domain (Jiawei Han and Pei, 2011; Witten et al., 2005). In addition, the data should be properly labeled and preprocessed to ensure that the machine learning algorithms can effectively learn from it.\nThe complexity of the problem is also a key factor in determining the success of machine learning (Gomez-Cabrero et al., 2014). Some problems are inherently more complex than others, and require more sophisticated algorithms and techniques to solve. For example, image recognition and natural language processing are typically more complex than simple regression problems.\nFinally, the choice of the appropriate algorithms and techniques is critical for the success of machine learning (Gomez-Cabrero et al., 2014; Sarker, 2021). Different algorithms and techniques are suited for different types of problems, and the choice of the appropriate one will depend on the specific problem and the available data. Additionally, the parameters and hyperparameters of the algorithms need to be properly tuned to ensure that the models are optimized for the problem at hand.\n\n\n\n\n\n\nNote\n\n\n\nDon’t miss out our next tutorial on Machine Learning (ML) using regression Algorithm!!!"
  },
  {
    "objectID": "posts/machine-learning-regression/index.html#data-loading-and-preprocessing",
    "href": "posts/machine-learning-regression/index.html#data-loading-and-preprocessing",
    "title": "Machine Learning (ML) using Regression Algorithm in R",
    "section": "",
    "text": "After loading the packages, load the penguins data (Horst et al., 2020) and remove all the missing values in the dataset. Name the dataset as penguin.data. The dataset consist of 333 rows and 8 columns\n\npenguin.data = palmerpenguins::penguins %&gt;% \n  drop_na()\n\npenguin.data\n\n# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 323 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThen, observe the internal structure of the dataset using glimpse() function of dplyr package. The dataset consist of 8 variables (columns); 3 are factors data (species, island and sex), 2 numeric or double data (bill_length_mm, bill_depth_mm) and 3 integers (flipper_length_mm, body_mass_g, year).\n\nglimpse(penguin.data)\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nIn this post we will use four variables from penguins data; which are bill_length_mm, bill_depth_mm, flipper_length_mm and body_mass_g. Use the select() function of dplyr package (Wickham et al., 2022) to select the variables of interest.\n\npenguin.data = penguin.data %&gt;% \n  select(flipper_length_mm, \n         bill_length_mm, \n         bill_depth_mm,\n         body_mass_g)\n\npenguin.data\n\n# A tibble: 333 × 4\n   flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n               &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;\n 1               181           39.1          18.7        3750\n 2               186           39.5          17.4        3800\n 3               195           40.3          18          3250\n 4               193           36.7          19.3        3450\n 5               190           39.3          20.6        3650\n 6               181           38.9          17.8        3625\n 7               195           39.2          19.6        4675\n 8               182           41.1          17.6        3200\n 9               191           38.6          21.2        3800\n10               198           34.6          21.1        4400\n# ℹ 323 more rows\n\n\nBefore we apply ML regression algorithms, first we will look on the distribution of the response variable (body_mass_g). Our response variable ranges from around 2700 to 6700 grams with the median value at around 4050 grams Figure 1.\n\npenguin.data %&gt;% \n  mutate(med.mass = median(body_mass_g, na.rm = T)) %&gt;% #median value = 4050\n  ggplot(aes(x = body_mass_g))+\n  geom_density(fill = \"cyan3\", alpha = 0.3)+\n  geom_vline(xintercept =  4050, color = \"red\", linetype = \"dashed\")+\n  theme_bw()\n\n\n\n\nFigure 1: The distribution of the body mass of Penguin species. The vertical dashed red line indicate the median body mass"
  },
  {
    "objectID": "posts/machine-learning-regression/index.html#data-spliting",
    "href": "posts/machine-learning-regression/index.html#data-spliting",
    "title": "Machine Learning (ML) using Regression Algorithm in R",
    "section": "",
    "text": "Regression algorithm as one of the supervised learning, needs two data types; the training and testing data set. The penguin.data will be split into two groups; training and testing dataset. The training dataset will have a proportion of 70% and the testing data will carry 30% of the total data. In total, our dataset has 333 observations in which 233 samples will be used to train the model while in validation of the accuracy of the model, we will use the testing set with 100 samples.\n\nset.seed(123)\nsplit.penguin = penguin.data %&gt;% \n  initial_split(prop = 0.7)\n\nsplit.penguin\n\n&lt;Training/Testing/Total&gt;\n&lt;233/100/333&gt;\n\n\nThe training data below with 233 samples will be used to training the model;\n\ntrain.set = split.penguin %&gt;% \n  training()\n\ntrain.set\n\n# A tibble: 233 × 4\n   flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n               &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;\n 1               230           59.6          17          6050\n 2               184           34.4          18.4        3325\n 3               215           45.2          15.8        5300\n 4               210           49            19.5        3950\n 5               202           41.4          18.5        3875\n 6               203           51            18.8        4100\n 7               212           44.9          13.8        4750\n 8               225           51.1          16.5        5250\n 9               210           50.8          19          4100\n10               211           45.4          14.6        4800\n# ℹ 223 more rows\n\n\nThe testing data with 100 observations will be used to test the accuracy of the model\n\ntest.set = split.penguin %&gt;% \n  testing()\n\ntest.set\n\n# A tibble: 100 × 4\n   flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n               &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;\n 1               186           39.5          17.4        3800\n 2               195           40.3          18          3250\n 3               195           38.7          19          3450\n 4               194           46            21.5        4200\n 5               189           35.9          19.2        3800\n 6               185           38.2          18.1        3950\n 7               188           39.5          17.8        3300\n 8               186           36            18.5        3100\n 9               191           42.3          21.2        4150\n10               186           39.6          17.7        3500\n# ℹ 90 more rows"
  },
  {
    "objectID": "posts/machine-learning-regression/index.html#model-specification",
    "href": "posts/machine-learning-regression/index.html#model-specification",
    "title": "Machine Learning (ML) using Regression Algorithm in R",
    "section": "",
    "text": "In order to perform the predictions on the body weight of the penguin species, we will use two different regression model types; The Random Forest and the Linear Regression model. The regression algorithm is applied when the response variable is numeric. We will look at one model after another and then at the end we will compare the best model among the two.\n\n\nFirst, we will specify the model type as random forest using the rand_forest() function of the parsnip package (Kuhn and Vaughan, 2022). The set engine ranger and the mode regression will be used.\nFinally, the model will be fitted with the train data set using the fit() function of the parsnip package (Kuhn and Vaughan, 2022) with the response variable as body_mass_g.\n\nmod.rf = rand_forest() %&gt;% \n  set_engine(engine = \"ranger\") %&gt;% \n  set_mode(mode = \"regression\") %&gt;% \n  fit(body_mass_g~., data = train.set)\n\nmod.rf\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      233 \nNumber of independent variables:  3 \nMtry:                             1 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       119311.9 \nR squared (OOB):                  0.8177082 \n\n\nThe summary of the fitted model shows the R-squared value is 0.80. This shows that 80% of our data fits better to the model.\n\n\nThen, we will evaluate the model’s performance on the testing data using the predict() function of stats package (R Core Team, 2022). The model have achieved predictions of body weight of the penguins using the test data set which was not used in data training. The values of the predictions are within the distribution ranges of the response variable.\n\npred.rf = mod.rf %&gt;% \n  predict(test.set) %&gt;% \n  bind_cols(test.set)\n\npred.rf\n\n# A tibble: 100 × 5\n   .pred flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n   &lt;dbl&gt;             &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;\n 1 3583.               186           39.5          17.4        3800\n 2 3833.               195           40.3          18          3250\n 3 3931.               195           38.7          19          3450\n 4 3980.               194           46            21.5        4200\n 5 3709.               189           35.9          19.2        3800\n 6 3693.               185           38.2          18.1        3950\n 7 3587.               188           39.5          17.8        3300\n 8 3509.               186           36            18.5        3100\n 9 4040.               191           42.3          21.2        4150\n10 3595.               186           39.6          17.7        3500\n# ℹ 90 more rows\n\n\n\n\n\nAfter validating the model performance, then we need to test its accuracy using the metrics() function of yardstick package (Kuhn et al., 2022). The data used will be the output of the trained model (pred.rf), the truth variable will be the actual values of the body mass and the estimate variable will be the the predicted values of the body mass.\n\nbb = pred.rf %&gt;% \n  metrics(truth = body_mass_g,\n          estimate = .pred)\n\nbb\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     343.   \n2 rsq     standard       0.815\n3 mae     standard     279.   \n\n\nThe accuracy result shows the R-squared value of 0.81 meaning that our model is accurate for more than 80%. Other performance indicators like Root Mean Squared Error (RMSE) and the Mean Absolute Error (MAE) are also low. The RMSE measures the average difference between values predicted by a model and the actual values while the MAE measures the average absolute error between actual and predicted values.\n\n\n\nFinally, we will use the tuned model to make predictions on new data. First we will create the new dataset using the rnorm() function of R software (R Core Team, 2022). We will name our new data as data.new.\n\ndata.new = tibble(bill_depth_mm = rnorm(n = 10,\n                                  mean = 15, \n                                  sd = 3),\n            bill_length_mm = rnorm(n = 10,\n                                   mean = 50, \n                                   sd = 10),\n            flipper_length_mm = rnorm(n = 10,\n                                      mean = 195,\n                                      sd = 50))\n\nmod.rf %&gt;% \n  predict(data.new) %&gt;% \n  bind_cols(data.new) %&gt;% \n  mutate(.pred = as.integer(.pred))\n\n# A tibble: 10 × 4\n   .pred bill_depth_mm bill_length_mm flipper_length_mm\n   &lt;int&gt;         &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;\n 1  5417          14.0           57.9              263.\n 2  3305          14.1           29.0              166.\n 3  4022          15.9           49.6              155.\n 4  3818          20.5           46.0              168.\n 5  5266          12.1           48.9              235.\n 6  4179          21.6           68.0              160.\n 7  3743          14.4           41.9              131.\n 8  5599          17.9           69.0              314.\n 9  4192          12.4           57.1              140.\n10  4745          13.5           57.4              205.\n\n\nThat’s great!!\nOur model has predicts the body mass for the penguin species for the new data.\n\n\n\n\nWhen using the linear regression model we will first specify the model type as Linear Regression using the linear_reg() function of the parsnip package (Kuhn and Vaughan, 2022). The set engine lm and the mode regression will be used.\nFinally, the model will be fitted with the train data set using the fit() function of the parsnip package (Kuhn and Vaughan, 2022) with the response variable as body_mass_g.\n\nmod.lm = linear_reg() %&gt;% \n  set_engine(engine = \"lm\") %&gt;% \n  set_mode(mode = \"regression\") %&gt;% \n  fit(body_mass_g~., data = train.set %&gt;% select(-flipper_length_mm))\n\nmod.lm\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = body_mass_g ~ ., data = data)\n\nCoefficients:\n   (Intercept)  bill_length_mm   bill_depth_mm  \n       2704.67           82.83         -123.51  \n\n\n\n\nThen, we will evaluate the model’s performance on the testing data using the predict() function of stats package (R Core Team, 2022). Our model has predicted for the body weight values of the penguins.\n\npred.lm = mod.lm %&gt;% \n  predict(test.set) %&gt;% \n  bind_cols(test.set)\n\npred.lm\n\n# A tibble: 100 × 5\n   .pred flipper_length_mm bill_length_mm bill_depth_mm body_mass_g\n   &lt;dbl&gt;             &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;\n 1 3827.               186           39.5          17.4        3800\n 2 3819.               195           40.3          18          3250\n 3 3563.               195           38.7          19          3450\n 4 3859.               194           46            21.5        4200\n 5 3307.               189           35.9          19.2        3800\n 6 3633.               185           38.2          18.1        3950\n 7 3778.               188           39.5          17.8        3300\n 8 3401.               186           36            18.5        3100\n 9 3590.               191           42.3          21.2        4150\n10 3798.               186           39.6          17.7        3500\n# ℹ 90 more rows\n\n\n\n\n\nThe model accuracy will be tested using the metrics() function of yardstick package (Kuhn et al., 2022).\n\ncc = pred.lm %&gt;% \n  metrics(truth = body_mass_g,\n          estimate = .pred)\n\ncc\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     616.   \n2 rsq     standard       0.411\n3 mae     standard     506.   \n\n\nThe accuracy result shows the R-square is 0.41, RMSE is 616.14 and the MAE value is 505.88."
  },
  {
    "objectID": "posts/machine-learning-regression/index.html#comparison-between-the-two-models",
    "href": "posts/machine-learning-regression/index.html#comparison-between-the-two-models",
    "title": "Machine Learning (ML) using Regression Algorithm in R",
    "section": "",
    "text": "We have seen the performance of each model. We have to choose which model is more accurate and performs better than the other. I will join the two accuracy test results; the one from the random forest and the other from the linear regression model (Table 1).\n\npred.rf %&gt;% \n  metrics(truth = body_mass_g,\n          estimate = .pred) %&gt;% \n  pivot_wider(names_from = .metric, \n              values_from = .estimate) %&gt;% \n  mutate(model = \"Random Forest\") %&gt;% \n  select(Model = 5, Rsquare=3, RMSE=2, MAE=4) %&gt;% \n  bind_rows(\n    \npred.lm %&gt;% \n  metrics(truth = body_mass_g,\n          estimate = .pred)  %&gt;% \n  pivot_wider(names_from = .metric, \n              values_from = .estimate) %&gt;% \n  mutate(model = \"Linear Regression\") %&gt;% \n  select(Model = 5, Rsquare=3, RMSE=2, MAE=4)\n) %&gt;% \n  mutate(across(is.numeric, round, 2))  %&gt;% \n  gt::gt()\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(is.numeric, round, 2)`.\nCaused by warning:\n! Use of bare predicate functions was deprecated in tidyselect 1.1.0.\nℹ Please use wrap predicates in `where()` instead.\n  # Was:\n  data %&gt;% select(is.numeric)\n\n  # Now:\n  data %&gt;% select(where(is.numeric))\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\n\n\n\nTable 1:  The table showing the model performance indicators \n  \n    \n    \n      Model\n      Rsquare\n      RMSE\n      MAE\n    \n  \n  \n    Random Forest\n0.81\n343.03\n279.11\n    Linear Regression\n0.41\n616.14\n505.88\n  \n  \n  \n\n\n\n\n\nThe table shows that the Random Forest is the best than the Linear Regression Model!! This is because it has higher R-squared value with low RMSE and MAE values (Table 1)."
  },
  {
    "objectID": "posts/machine-learning-regression/index.html#summary",
    "href": "posts/machine-learning-regression/index.html#summary",
    "title": "Machine Learning (ML) using Regression Algorithm in R",
    "section": "",
    "text": "All in all, the success of machine learning and artificial intelligence depend on the quality of the data, the complexity of the problem, and the choice of the appropriate algorithms and techniques (Jiawei Han and Pei, 2011; Witten et al., 2005).\nThe quality of the data is one of the most important factors for the success of machine learning. The data should be accurate, complete, and representative of the problem domain (Jiawei Han and Pei, 2011; Witten et al., 2005). In addition, the data should be properly labeled and preprocessed to ensure that the machine learning algorithms can effectively learn from it.\nThe complexity of the problem is also a key factor in determining the success of machine learning (Gomez-Cabrero et al., 2014). Some problems are inherently more complex than others, and require more sophisticated algorithms and techniques to solve. For example, image recognition and natural language processing are typically more complex than simple regression problems.\nFinally, the choice of the appropriate algorithms and techniques is critical for the success of machine learning (Gomez-Cabrero et al., 2014; Sarker, 2021). Different algorithms and techniques are suited for different types of problems, and the choice of the appropriate one will depend on the specific problem and the available data. Additionally, the parameters and hyperparameters of the algorithms need to be properly tuned to ensure that the models are optimized for the problem at hand.\n\n\n\n\n\n\nNote\n\n\n\nDon’t miss out our next post in this blog!!!"
  },
  {
    "objectID": "posts/worldcloud/index.html#tidy-project-titles",
    "href": "posts/worldcloud/index.html#tidy-project-titles",
    "title": "Text analytics and word cloud in R and ggplot2",
    "section": "tidy project titles",
    "text": "tidy project titles\n\nstudent.text = student$Title \n\nThereafter, a corpus will be generated from the vector using the Corpus() function of tm package\n\nstudent.doc = tm::Corpus(VectorSource(student.text))\n\nCleaning is an essential step to take before you generate your wordcloud. Indeed, for your analysis to bring useful insights, you may want to remove special characters, numbers or punctuation from your text. In addition, you should remove common stop words in order to produce meaningful results and avoid the most common frequent words such as “I” or “the” to appear in the word cloud. If you’re working with a corpus, there are several packages you can use to clean your text. The following lines of code show you how to do this using the tm package.\n\nstudent.doc.clean = student.doc %&gt;% \n  tm_map(removeNumbers) %&gt;% \n  tm_map(removePunctuation) %&gt;% \n  tm_map(stripWhitespace) %&gt;% \n  tm_map(content_transformer(tolower)) %&gt;% \n  tm_map(removeWords, stopwords(kind = \"en\"))\n\n\nCreate a document-term-matrix\nWhat you want to do as a next step is to have a dataframe containing each word in your first column and their frequency in the second column. This can be done by creating a document term matrix with the TermDocumentMatrix function from the tm package.\n\nstudent.matrix = TermDocumentMatrix(student.doc.clean) %&gt;% \n  as.matrix()\n\nstudent.word = sort(rowSums(student.matrix), decreasing = TRUE)\n     \n student.word.df = data.frame(word = names(student.word), freq = student.word)\n\n\n\nGenerate the word cloud\nThe wordcloud package is the most classic way to generate a word cloud. The following line of code shows you how to properly set the arguments. As an example, I chose to work with the student titles on their third year research project at SoAF.\n\nset.seed(1234) # for reproducibility\n\nwordcloud(words = student.word.df$word, \n          freq = student.word.df$freq, \n          min.freq = 1,\n          random.order = FALSE, \n         max.words = 200,\n         # rot.per = 0.35,\n          colors = brewer.pal(8, \"Dark2\"),\n         scale = c(3.5,0.4))\n\n\n\n\nIt may happen that your word cloud crops certain words or simply doesn’t show them. If this happens, make sure to add the argument scale=c(3.5,0.25) and play around with the numbers to make the word cloud fit. Another common mistake with word clouds is to show too many words that have little frequency. If this is the case, make sure to adjust the minimum frequency argument (min.freq=…) in order to render your word cloud more meaningful.\n\nset.seed(1234) # for reproducibility\n\nwordcloud(words = student.word.df$word, \n          freq = student.word.df$freq, \n          min.freq = 3,\n          random.order = FALSE, \n         # max.words = 1000,\n         # rot.per = 0.35,\n          colors = brewer.pal(8, \"Dark2\"), \n         scale = c(3.5,0.6))\n\n\n\n\nThe wordcloud2 package is a bit more fun to use, allowing us to do some more advanced visualisations. For instance, you can choose your wordcloud to appear in a specific shape or even letter (see this vignette for a useful tutorial). As an example, I used the same corpus of student title and generated the two word clouds shown below. Cool, right?\n\nwordcloud2(data = student.word.df, \n           size = 0.8, \n           color = \"random-dark\",\n           minSize = 3,\n           ellipticity = 2,\n           gridSize = 11)\n\n\n\n\n\nIn summary, R language provide myriad package for visualizing not only numeric data but also for textual information and summarize them in plots that are easily to understand. In the next post, we are going to use the approach to visualize textual information using alluvial plots"
  },
  {
    "objectID": "posts/remove-largefile-staged-git/index.html",
    "href": "posts/remove-largefile-staged-git/index.html",
    "title": "How to Remove a Large Staged File from Git",
    "section": "",
    "text": "Introduction\nWhen working with Git, it is common to accidentally stage large files that should not be included in the repository. These large files can bloat the repository and make it difficult to work with or sometimes the push fails. When the push fails, it will display an error message that shows the name of that large file in your repository. When this happens, there are steps you can take to remove these large staged files from Git.\n\n\nSteps in removing a file from Git\n\nIdentify the Large Staged File: Before removing the large staged file, you need to identify which file is causing the issue, or read the error message and identify the file from your repository.\nOpen the Git bash and move to your working directory using cd command. \nUnstage the File: Once you have identified the large staged file and moved to your working directory, you can unstage it using the git reset &lt;file&gt; command. In this case, I will remove mtwara_sst_june.csv file from the repository.  This will remove the file from the staging area, but it will still be present in your working directory in the .git folder that was generated during commit.\nRemove the File from Git History: To completely remove the large staged file from Git, you will need to delete the .git folder from your working directory. \nRe-push Changes to your Repository: After removing the large staged file from Git, you will need to make changes to your repository. Since it is not your first commit, then you will only run three steps to make those changes to your GitHub repository; add, commit and push.\n\n\nAdd all your files in the working directory using git add .\n\n\n\nCommit changes to your repository using git commit -m “Add commit message”\n\n\n\nPush your work using git push\n\n\n\nIf it still give the same error message that there is large file, the only option you have is to delete that repository from GitHub and you may re-create a new repository using the same name but by changing the working directory.\nCopy all your files except the .git folder from the older folder to the new folder.\nInitiate Git in your local development environment by opening the GitBash and follow the deployment steps to GitHub.\n\nAfter deployment, navigate to your repository on GitHub and verify that the changes have been deployed successfully.\nIf the changes made to your GitHub repository are successful, you may wish to publish your repository and make it accessible to the public as a web page"
  },
  {
    "objectID": "posts/remove-largefile-staged-git/index.html#consultated-references",
    "href": "posts/remove-largefile-staged-git/index.html#consultated-references",
    "title": "How to Remove a Large Staged File from Git",
    "section": "Consultated references",
    "text": "Consultated references"
  },
  {
    "objectID": "posts/cropping-raster-terra-tidyterra/index.html",
    "href": "posts/cropping-raster-terra-tidyterra/index.html",
    "title": "Cropping raster image using terra and tidyterra packages in R",
    "section": "",
    "text": "Raster data is a type of spatial data that is stored as a grid of squares or rectangles called pixels. Each pixel contains a value that represents a specific location on the Earth’s surface (Hijmans, 2024). Terra package provides methods to manipulate spatial data in both raster and vector format (Hijmans, 2024). Terra package supports SpatRaster data – that can handle large raster files and SpatVector data – that supports all types of geometric operations such as intersections in vector files. Tidyterra is a package that uses common methods from the tidyverse for SpatRaster and SpatVector objects created with the terra package (Hernangómez, 2023).\nFirst, we will load packages that we are going to use in this post:\n\nrequire(tidyterra)\nrequire(terra)\nrequire(leaflet)"
  },
  {
    "objectID": "posts/extract-values-raster-terra/index.html",
    "href": "posts/extract-values-raster-terra/index.html",
    "title": "Extracting values from raster image using terra package in R",
    "section": "",
    "text": "When working with raster images, you may need to obtain the values of certain points from the raster for further analysis. This is often achieved by extracting specific values at particular points. The R packages terra (Hijmans, 2024) and tidyterra (Hernangómez, 2023) provide tools to efficient extract these values, whether you generate random points or have pre-defined locations. Below is a simplified overview of the process:\nBefore starting, ensure you have both terra and tidyterra installed and loaded into your R environment. Other packages such as sf also will be used in this post.\n\nrequire(tidyterra)\nrequire(terra)\nrequire(sf)\nrequire(tidyverse)"
  },
  {
    "objectID": "posts/extract-values-raster-terra/index.html#plotting-points",
    "href": "posts/extract-values-raster-terra/index.html#plotting-points",
    "title": "Extracting values from raster image using terra package in R",
    "section": "Plotting points",
    "text": "Plotting points\nNext, we will visualize our points on a map to observe their distribution. As shown in Figure 1, all our randomly generated points fall within the water, with none located on land.\n\nggplot()+\n  ggspatial::annotation_map_tile(type = \"osm\", zoom = 8)+\n  ggspatial::layer_spatial(data = pemba.data)+\n  scale_x_continuous(breaks = seq(39.2,40,0.4), \n                     labels = metR::LonLabel(lon = seq(39.2,40,0.4)))+\n  scale_y_continuous(breaks = seq(-5.6,-4.6,0.4),\n                     labels = metR::LatLabel(lat = seq(-5.6,-4.6,0.4)))\n\n\n\n\nFigure 1: The map of Pemba Channel showing the generated random points\n\n\n\n\nThe extracted spatvector points include chlorophyll values for each point, as illustrated in Figure 2.\n\npemba.data |&gt; \n  ggplot() +\n  geom_spatvector(aes(color = chl, size = chl))+\n  scale_x_continuous(breaks = seq(39.2,40,0.4), \n                     labels = metR::LonLabel(lon = seq(39.2,40,0.4)))+\n  scale_y_continuous(breaks = seq(-5.6,-4.6,0.4),\n                     labels = metR::LatLabel(lat = seq(-5.6,-4.6,0.4)))+\n  theme_bw() \n\n\n\n\nFigure 2: The value of chlorophyll-a extracted from each generated random point"
  },
  {
    "objectID": "posts/github-site-as-web-page/index.html",
    "href": "posts/github-site-as-web-page/index.html",
    "title": "How to create GitHub site as a web page",
    "section": "",
    "text": "Introduction\nCreating a GitHub site as a web page is an excellent way to host your personal website, project site, or documentation directly from your GitHub repository. This method allows you to share your work with others easily, using a reliable and well-maintained infrastructure in GitHub. Whether you are a developer sharing codes, a designer displaying a portfolio, or simply need a platform to publish documentation, GitHub Pages offers a simple and effective solution. By following a few easy steps, you can set up a professional-looking website that is hosted for free, without worrying about server management or back-end coding. This approach not only enhances your online presence but also makes it easy to keep your site updated and aligned with your projects. Whether you are a new to this or have experience, creating a GitHub site is a useful way to showcase your work and enhance your professional profile. Before beginning these steps, ensure that you have a GitHub account and an existing repository. If you do not have a GitHub account yet, you can create one at the GitHub website. For guidance on creating and deploying a GitHub repository, refer to this tutorial on creating and deploying a GitHub repository.\n\n\nNavigate to GitHub repository\nLog in to your GitHub account and navigate to the specific repository that you wish to turn into a web page. This repository should contain the files you want to showcase on your site, such as index.html, index.md, README.md, or any other web assets. If you do not have a repository prepared, you can create a new one by clicking on the New Repository button. Once you are in the repository, you will be ready to start the process of configuring it for GitHub Pages, which will allow you to easily host and share your web content online. Take a moment to review your repository to ensure that all necessary files are in place before proceeding to the next steps.\n\nIn your repository, navigate to the Settings tab by clicking on it. This will take you to a page where you can configure various options for your repository, including those needed to set up GitHub Pages. The Settings tab is typically located near the top right of the repository page, alongside other options like Code, Issues, Pull requests, Actions and other tabs.\n\nUnder the Settings tab, scroll down and select Pages from the sidebar menu. This action will open the GitHub Pages settings window, where you can configure the options for publishing your website. In this section, you will be able to choose the source for your site, such as the branch and folder where your website files are stored, and customize other settings related to your GitHub Pages site.\n\nIn the GitHub Pages settings, locate the drop-down menu under the Branch section. Click on the drop-down arrow, and select main as the branch from which your site will be built. This tells GitHub Pages to use the content from the main branch of your repository to generate and host your website.\nIn the same Branch drop-down menu, after selecting main as the branch, look for the option to specify a folder within the branch. Select the /docs folder as the directory from which your site will be built. This means that GitHub Pages will use the files located in the /docs folder of your main branch to create your website. If your web content is stored in this folder, GitHub will look for an index.html, index.md, or README.md file as the entry file for your site.\n\nAfter selecting the main branch and the /docs folder as a source for your site , click the Save button to apply these settings. This will initiate the process of setting up your GitHub Pages site. Once the settings are saved, you will see a confirmation message, and the link to your live site will appear under the GitHub Pages section of your repository settings. You can visit your newly created website by clicking on the Visit site link. This link will take you directly to your published site, which is now accessible to anyone on the web.\n\nCongratulations on setting up your GitHub Pages site!\nWith your repository now configured to serve as a live web page, you have a professional platform to show your projects, personal website, or documentation of your work."
  }
]